# Source Discovery - Auto-configuraci√≥n de Fuentes

## Objetivo

Permitir a los usuarios configurar fuentes de informaci√≥n de forma autom√°tica proporcionando:
- Contexto geogr√°fico (ej: "√Ålava", "Pa√≠s Vasco")
- Tem√°tica (ej: "pol√≠tica local", "cultura")
- Tipo de fuente (ej: "ayuntamientos", "medios oficiales")

El sistema debe descubrir fuentes p√∫blicas accesibles sin copyright autom√°ticamente.

---

## Opci√≥n 1: LLM + Web Search Discovery (M√ÅS INTELIGENTE)

### Descripci√≥n
Usar LLM con capacidad de web search para descubrir fuentes en tiempo real.

### Flujo
```
Usuario: "Noticias de √Ålava"
‚Üì
1. LLM genera keywords: "√°lava ayuntamientos sitios oficiales noticias"
2. Web search (Perplexity/Groq Compound) busca fuentes
3. LLM analiza resultados y filtra:
   ‚úÖ Sitios oficiales (.eus, .gob.es, ayuntamientos)
   ‚úÖ Medios locales conocidos
   ‚úÖ RSS/Atom disponibles
   ‚ùå Descarta: paywalls, copyright, redes sociales
4. LLM detecta estructura (RSS, lista de noticias, etc.)
5. Propone fuentes con config autodetectada
```

### Implementaci√≥n
```python
async def discover_with_llm(criteria: str) -> List[Dict]:
    """Discover sources using LLM with web search."""

    prompt = f"""
    Busca fuentes de noticias p√∫blicas para: {criteria}

    Criterios:
    - Solo sitios oficiales o medios verificados
    - Sin paywall ni copyright restrictivo
    - Preferir .eus, .gob.es, ayuntamientos, diputaciones
    - Indicar si tiene RSS/Atom

    Devuelve JSON:
    [
      {{
        "name": "Ayuntamiento de Vitoria",
        "url": "https://vitoria-gasteiz.org/noticias",
        "rss_url": "https://...",
        "type": "ayuntamiento",
        "tags": ["alava", "vitoria", "oficial"],
        "confidence": 0.95
      }}
    ]
    """

    # Use Groq Compound or Perplexity with web search
    response = await groq_compound.ainvoke(prompt)

    sources = parse_json_response(response)

    # Validate URLs (HEAD request)
    validated = await validate_sources(sources)

    return validated
```

### Ventajas
- ‚úÖ Muy flexible, funciona para cualquier zona geogr√°fica
- ‚úÖ Descubre fuentes nuevas autom√°ticamente
- ‚úÖ Detecta estructura y tipo de scraping necesario

### Desventajas
- ‚ùå Coste LLM por b√∫squeda
- ‚ùå Puede encontrar fuentes inestables
- ‚ùå Requiere validaci√≥n posterior

---

## Opci√≥n 2: Directorio Curado + Matching Sem√°ntico (M√ÅS FIABLE)

### Descripci√≥n
Mantener un cat√°logo pre-verificado de fuentes y usar b√∫squeda sem√°ntica para matching.

### Schema
```sql
CREATE TABLE source_catalog (
  id UUID PRIMARY KEY,
  name VARCHAR NOT NULL,
  url VARCHAR NOT NULL,
  source_type VARCHAR, -- 'scraping', 'rss', 'api'
  tags TEXT[], -- ["euskadi", "alava", "vitoria", "ayuntamiento"]
  geo_scope TEXT, -- "Vitoria-Gasteiz", "√Ålava", "Euskadi"
  topics TEXT[], -- ["pol√≠tica local", "urbanismo", "cultura"]
  verified BOOLEAN DEFAULT false,
  config_template JSONB, -- Pre-configuraci√≥n probada
  embedding VECTOR(768), -- Para b√∫squeda sem√°ntica
  created_at TIMESTAMPTZ,
  last_validated TIMESTAMPTZ
);

-- √çndices
CREATE INDEX idx_source_catalog_tags ON source_catalog USING GIN(tags);
CREATE INDEX idx_source_catalog_embedding ON source_catalog
  USING ivfflat (embedding vector_cosine_ops);
```

### Flujo
```
Usuario: "Pol√≠tica local en Vitoria"
‚Üì
1. Generar embedding de la consulta
2. B√∫squeda sem√°ntica en source_catalog
3. Filtro adicional por tags/geo_scope
4. Ranking por relevancia
5. Muestra top 10 fuentes sugeridas
6. Usuario selecciona ‚Üí auto-configura con config_template
```

### Implementaci√≥n
```python
async def search_catalog(criteria: str, limit: int = 10) -> List[Dict]:
    """Search curated source catalog."""

    # Generate embedding for query
    embedding = await get_embedding(criteria)

    # Semantic search + tag filtering
    results = supabase.client.rpc('search_sources', {
        'query_embedding': embedding,
        'match_count': limit,
        'filter_tags': extract_tags(criteria)  # ["alava", "vitoria"]
    }).execute()

    return results.data
```

### Cat√°logo Inicial (Euskadi)
```yaml
sources:
  - name: Diputaci√≥n Foral de √Ålava
    url: https://prentsa.araba.eus/es/noticias
    tags: [alava, euskadi, diputacion, oficial]
    geo_scope: √Ålava
    verified: true

  - name: Ayuntamiento de Vitoria-Gasteiz
    url: https://www.vitoria-gasteiz.org/noticias
    tags: [alava, vitoria, ayuntamiento, oficial]
    geo_scope: Vitoria-Gasteiz
    verified: true

  - name: Ayuntamiento de Aiara
    url: https://www.aiaraldea.eus/noticias
    tags: [alava, aiara, ayuntamiento, oficial]
    geo_scope: Aiara
    verified: true

  # ... ~50-100 fuentes m√°s
```

### Ventajas
- ‚úÖ Fuentes verificadas y estables
- ‚úÖ Configuraci√≥n probada (config_template)
- ‚úÖ R√°pido (b√∫squeda local)
- ‚úÖ Sin coste LLM por b√∫squeda

### Desventajas
- ‚ùå Requiere curadur√≠a manual inicial
- ‚ùå No descubre fuentes nuevas autom√°ticamente
- ‚ùå Limitado al cat√°logo existente

---

## Opci√≥n 3: Pattern Detection Autom√°tico (M√ÅS T√âCNICO)

### Descripci√≥n
Usuario provee URLs de ejemplo, el sistema detecta patrones y genera fuentes similares.

### Flujo
```
Usuario provee ejemplos:
- vitoria-gasteiz.org/noticias
- donostia.eus/actualidad
- bilbao.eus/prensa
‚Üì
1. LLM detecta patr√≥n: "{municipio}.{tld}/{seccion}"
2. Busca lista de municipios en OpenData Euskadi
3. Genera URLs candidatas autom√°ticamente:
   - eibar.eus/actualidad
   - getxo.eus/noticias
   - barakaldo.eus/prensa
4. Verifica cada URL (HEAD request 200)
5. Test de scraping b√°sico (detecta lista de noticias)
6. Propone batch de fuentes v√°lidas
```

### Implementaci√≥n
```python
async def discover_by_pattern(example_urls: List[str]) -> List[Dict]:
    """Discover sources by pattern detection."""

    # 1. Detect pattern with LLM
    pattern = await detect_url_pattern(example_urls)
    # ‚Üí "{municipality}.eus/noticias"

    # 2. Get list of municipalities from OpenData
    municipalities = await fetch_opendata_euskadi(
        "https://opendata.euskadi.eus/api/datasets/municipios"
    )

    # 3. Generate candidate URLs
    candidates = []
    for muni in municipalities:
        url = pattern.format(
            municipality=muni['slug'],
            tld=muni.get('domain_tld', 'eus')
        )
        candidates.append({
            "name": f"Ayuntamiento de {muni['name']}",
            "url": url,
            "geo_scope": muni['comarca']
        })

    # 4. Validate URLs (parallel HEAD requests)
    valid_urls = await validate_urls_batch(candidates)

    # 5. Test scraping structure
    scrapeable = await test_scraping_batch(valid_urls, max_concurrent=10)

    return scrapeable
```

### Ventajas
- ‚úÖ Escala bien para estructuras repetitivas
- ‚úÖ Descubre muchas fuentes de golpe
- ‚úÖ √ötil para ayuntamientos con estructura com√∫n

### Desventajas
- ‚ùå Solo funciona si hay patr√≥n com√∫n
- ‚ùå No todos los ayuntamientos siguen el patr√≥n
- ‚ùå Requiere validaci√≥n posterior

---

## Opci√≥n 4: Open Data + Institutional APIs (M√ÅS ESTRUCTURADO)

### Descripci√≥n
Usar APIs oficiales como fuente de metadatos para descubrir instituciones.

### Fuentes de Datos
```yaml
apis:
  - OpenData Euskadi:
      url: https://opendata.euskadi.eus/api
      datasets:
        - Ayuntamientos y municipios
        - Diputaciones forales
        - Organismos p√∫blicos
        - Boletines oficiales (BOPV)

  - Wikidata:
      url: https://query.wikidata.org/sparql
      queries:
        - Medios de comunicaci√≥n vascos
        - Instituciones p√∫blicas Euskadi
        - Sitios web oficiales

  - Wikipedia:
      url: https://es.wikipedia.org/w/api.php
      content:
        - Lista de medios locales
        - Enlaces a sitios oficiales
```

### Flujo
```
Usuario: "Ayuntamientos de Bizkaia"
‚Üì
1. Query a OpenData Euskadi API:
   GET /datasets/ayuntamientos?provincia=bizkaia

2. Para cada ayuntamiento:
   a) Construir URL probable: {nombre}.eus
   b) Verificar existencia (HEAD request)
   c) Crawl homepage para encontrar secci√≥n noticias
   d) Buscar RSS/Atom feeds

3. Extraer metadata:
   - T√≠tulo de la web
   - Enlaces a secciones (noticias, actualidad, prensa)
   - Feeds disponibles

4. Proponer fuentes auto-configuradas
```

### Implementaci√≥n
```python
async def discover_from_opendata(
    criteria: Dict[str, Any]
) -> List[Dict]:
    """Discover sources from OpenData Euskadi."""

    # Query OpenData API
    institutions = await fetch_opendata_euskadi(
        dataset="ayuntamientos",
        filters=criteria  # {"provincia": "bizkaia"}
    )

    sources = []
    for inst in institutions:
        # Try standard patterns
        base_urls = [
            f"https://{inst['slug']}.eus",
            f"https://{inst['slug']}.org",
            f"https://www.{inst['slug']}.eus"
        ]

        for base_url in base_urls:
            # Check if exists
            if await url_exists(base_url):
                # Crawl for news section
                news_urls = await find_news_section(base_url)

                # Check for RSS
                rss_urls = await find_rss_feeds(base_url)

                sources.append({
                    "name": inst['name'],
                    "url": news_urls[0] if news_urls else base_url,
                    "rss_url": rss_urls[0] if rss_urls else None,
                    "tags": [inst['provincia'].lower(), inst['comarca'].lower()],
                    "geo_scope": inst['comarca']
                })
                break

    return sources
```

### Ventajas
- ‚úÖ Datos oficiales, alta calidad
- ‚úÖ Coverage completo de instituciones
- ‚úÖ Metadata estructurada

### Desventajas
- ‚ùå APIs limitadas (no todos tienen endpoint)
- ‚ùå No todos tienen web estructurada
- ‚ùå Requiere crawling adicional

---

## Opci√≥n 5: Hybrid - Cat√°logo + LLM Discovery (RECOMENDADO)

### Descripci√≥n
Combinar cat√°logo curado (r√°pido, fiable) con LLM discovery (flexible).

### Arquitectura en Capas

**Tier 1 - Cat√°logo Curado** (inmediato):
- 100-200 fuentes verificadas de Euskadi
- Config probada, alta calidad
- B√∫squeda sem√°ntica instant√°nea

**Tier 2 - LLM Discovery** (bajo demanda):
- Si no hay suficientes matches en cat√°logo
- LLM busca + valida + propone
- Si funciona ‚Üí a√±adir a cat√°logo

**Tier 3 - Community Sourced** (futuro):
- Usuarios pueden sugerir fuentes
- Review + validation autom√°tica
- Aprobaci√≥n ‚Üí promoci√≥n a Tier 1

### Flujo Completo
```
Usuario: "Noticias de econom√≠a en Gipuzkoa"
‚Üì
1. B√∫squeda en cat√°logo (Tier 1)
   ‚Üí Encuentra 3 fuentes: Diputaci√≥n Gipuzkoa, Bilbao Ekonomia, ...

2. Si <5 resultados ‚Üí LLM Discovery (Tier 2)
   ‚Üí Groq Compound busca m√°s fuentes
   ‚Üí Valida URLs y scraping
   ‚Üí A√±ade 4 fuentes nuevas

3. Ranking combinado:
   - Tier 1: relevancia + verified=true ‚Üí boost
   - Tier 2: relevancia + confidence score

4. Presenta top 10 al usuario

5. Usuario selecciona 5 fuentes
   ‚Üí Auto-configuraci√≥n con config_template
   ‚Üí Fuentes Tier 2 exitosas ‚Üí marcar para revisi√≥n (‚Üí Tier 1)
```

### Implementaci√≥n
```python
@app.post("/api/v1/sources/discover")
async def discover_sources(
    request: SourceDiscoveryRequest,
    user: Dict = Depends(get_current_user_from_jwt)
):
    """Hybrid source discovery: Catalog + LLM."""

    criteria = request.criteria  # "econom√≠a gipuzkoa"
    min_results = request.min_results or 10

    # 1. Search catalog (Tier 1)
    catalog_matches = await search_catalog(
        criteria=criteria,
        limit=min_results
    )

    logger.info("catalog_search_results", count=len(catalog_matches))

    # 2. If insufficient, use LLM discovery (Tier 2)
    if len(catalog_matches) < min_results:
        llm_discovered = await discover_with_llm(
            criteria=criteria,
            existing_urls=[s['url'] for s in catalog_matches]
        )

        # Validate discovered sources
        validated = await validate_discovered_sources(llm_discovered)

        # Add to results (lower ranking than catalog)
        catalog_matches.extend(validated)

        logger.info("llm_discovery_results",
            discovered=len(llm_discovered),
            validated=len(validated)
        )

    # 3. Rank and return
    ranked = rank_sources(catalog_matches, criteria)

    return {
        "sources": ranked[:min_results],
        "total": len(ranked),
        "catalog_count": len([s for s in ranked if s.get('verified')]),
        "discovered_count": len([s for s in ranked if not s.get('verified')])
    }
```

### Ventajas
- ‚úÖ Mejor de ambos mundos
- ‚úÖ R√°pido para casos comunes (cat√°logo)
- ‚úÖ Flexible para casos raros (LLM)
- ‚úÖ El cat√°logo crece con el uso

### Desventajas
- ‚ùå M√°s complejo de implementar
- ‚ùå Requiere curadur√≠a inicial del cat√°logo

---

## Implementaci√≥n Recomendada (Roadmap)

### Fase 1: MVP - Cat√°logo Curado (2 semanas)

**Tareas:**
1. Crear tabla `source_catalog` en Supabase
2. Curar ~50 fuentes de Euskadi manualmente:
   - 3 Diputaciones
   - ~25 Ayuntamientos principales
   - ~10 Medios locales oficiales
   - ~10 Organismos (Euskalmet, universidades, etc.)
3. Implementar b√∫squeda por tags
4. Frontend: Wizard b√°sico de selecci√≥n

**Deliverable:**
```
POST /api/v1/sources/discover
Body: { "criteria": "ayuntamientos √°lava" }
Response: { "sources": [...], "total": 15 }
```

### Fase 2: LLM Discovery (2 semanas)

**Tareas:**
1. Integrar Groq Compound para web search
2. Implementar validaci√≥n de URLs
3. Test de scraping autom√°tico
4. Hybrid search (cat√°logo + LLM)

**Deliverable:**
- Discovery funciona para cualquier criterio
- Valida URLs antes de proponer
- Auto-a√±ade fuentes exitosas al cat√°logo

### Fase 3: Pattern Detection (opcional)

**Tareas:**
1. Integraci√≥n con OpenData Euskadi
2. Pattern detection por ejemplos
3. Batch discovery de ayuntamientos

### Fase 4: Community & Auto-improvement

**Tareas:**
1. Usuarios pueden sugerir fuentes
2. Auto-validaci√≥n peri√≥dica de cat√°logo
3. Machine learning para mejorar ranking

---

## UI/UX - Wizard de Discovery

### Paso 1: Criterios
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Descubre Fuentes Autom√°ticamente       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  üìç Ubicaci√≥n:                          ‚îÇ
‚îÇ  [x] √Ålava  [ ] Bizkaia  [ ] Gipuzkoa  ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üèõÔ∏è  Tipo de Fuente:                    ‚îÇ
‚îÇ  [x] Ayuntamientos                      ‚îÇ
‚îÇ  [x] Diputaciones                       ‚îÇ
‚îÇ  [ ] Medios Locales                     ‚îÇ
‚îÇ  [ ] Organismos P√∫blicos                ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üì∞ Temas:                              ‚îÇ
‚îÇ  [x] Pol√≠tica Local                     ‚îÇ
‚îÇ  [ ] Cultura                            ‚îÇ
‚îÇ  [ ] Deportes                           ‚îÇ
‚îÇ  [ ] Econom√≠a                           ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üîç Texto libre (opcional):             ‚îÇ
‚îÇ  [________________________]             ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ          [Buscar Fuentes] ‚Üí             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Paso 2: Resultados
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Encontradas 12 fuentes                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ ‚òë Ayuntamiento de Vitoria-Gasteiz      ‚îÇ
‚îÇ   üåê vitoria-gasteiz.org/noticias       ‚îÇ
‚îÇ   ‚úì Verificada  üìä 100 noticias/mes     ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ ‚òë Diputaci√≥n Foral de √Ålava             ‚îÇ
‚îÇ   üåê prentsa.araba.eus                  ‚îÇ
‚îÇ   ‚úì Verificada  üìä 50 noticias/mes      ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ ‚òê Ayuntamiento de Aiara                 ‚îÇ
‚îÇ   üåê aiaraldea.eus/noticias             ‚îÇ
‚îÇ   ‚ö° Descubierta  üìä ~20 noticias/mes    ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ [Ver 9 m√°s...]                          ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ     [Cancelar]  [Configurar 2 ‚Üí]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Paso 3: Configuraci√≥n
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Configurar 2 fuentes seleccionadas     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  ‚è∞ Frecuencia de actualizaci√≥n:         ‚îÇ
‚îÇ  ‚óã Cada hora                            ‚îÇ
‚îÇ  ‚óè Cada 2 horas (recomendado)           ‚îÇ
‚îÇ  ‚óã Cada 6 horas                         ‚îÇ
‚îÇ  ‚óã Diaria                               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üéØ Prioridad:                          ‚îÇ
‚îÇ  ‚óè Alta  ‚óã Media  ‚óã Baja               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üè∑Ô∏è  Etiquetas adicionales:              ‚îÇ
‚îÇ  [gobierno local] [+]                   ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ         [‚Üê Volver]  [Activar ‚Üí]         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## M√©tricas de √âxito

**KPIs a trackear:**
1. Tiempo medio de configuraci√≥n de fuentes: `<5 minutos` (vs ~30 min manual)
2. % fuentes v√°lidas descubiertas: `>80%`
3. % fuentes que siguen activas a 30 d√≠as: `>90%`
4. Noticias capturadas por fuente/d√≠a: `>5`
5. Satisfacci√≥n del usuario: `>4/5`

---

## Pr√≥ximos Pasos

1. **Decisi√≥n**: ¬øOpci√≥n 2 (Cat√°logo) o Opci√≥n 5 (Hybrid)?
2. **Prototipo**: Implementar MVP de discovery endpoint
3. **Cat√°logo**: Curar primeras 50 fuentes de Euskadi
4. **Frontend**: Wizard b√°sico de selecci√≥n
5. **Testing**: Validar con usuarios beta
