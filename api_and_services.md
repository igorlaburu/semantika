# Documentaci√≥n API y Servicios - Sistema Ekimen

**Versi√≥n**: 0.2.0
**√öltima actualizaci√≥n**: 2025-11-13
**Base URL**: `https://api.ekimen.ai`

---

## Tabla de Contenidos

1. [Arquitectura del Sistema](#arquitectura-del-sistema)
2. [Autenticaci√≥n](#autenticaci√≥n)
3. [Gesti√≥n de Usuarios y Clientes](#gesti√≥n-de-usuarios-y-clientes)
4. [Endpoints de API](#endpoints-de-api)
5. [Servicios de Procesamiento](#servicios-de-procesamiento)
6. [Workflows y Tareas Programadas](#workflows-y-tareas-programadas)
7. [Monitores Autom√°ticos](#monitores-autom√°ticos)
8. [Crear Workflows Personalizados](#crear-workflows-personalizados)
9. [Configuraci√≥n del Sistema](#configuraci√≥n-del-sistema)
10. [Uso y Facturaci√≥n](#uso-y-facturaci√≥n)

---

## Arquitectura del Sistema

El sistema **ekimen** es una plataforma multi-tenant para procesamiento sem√°ntico de datos, compuesta por:

### Componentes Principales

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        FRONTEND                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              Kazet (Cliente Web)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ HTTPS + X-API-Key
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        BACKEND                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ semantika-api   ‚îÇ  ‚îÇ semantika-       ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ (FastAPI)       ‚îÇ  ‚îÇ scheduler        ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ Puerto 8000     ‚îÇ  ‚îÇ (APScheduler)    ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ           ‚îÇ                     ‚îÇ                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                     ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ               ‚îÇ                    ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇSupabase‚îÇ   ‚îÇ   Qdrant    ‚îÇ   ‚îÇ  OpenRouter   ‚îÇ   ‚îÇExternal‚îÇ
‚îÇ(Config)‚îÇ   ‚îÇ  (Vectores) ‚îÇ   ‚îÇ    (LLMs)     ‚îÇ   ‚îÇ APIs   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Servicios Docker

1. **semantika-api**: API REST principal (FastAPI + Uvicorn)
2. **semantika-scheduler**: Daemon para tareas programadas (APScheduler)

### Stack Tecnol√≥gico

- **Backend**: Python 3.10+, FastAPI, APScheduler
- **Base de Datos**: Supabase (PostgreSQL + pgvector)
- **Vector Store**: Qdrant Cloud
- **LLM**: OpenRouter (Claude 3.5 Sonnet, GPT-4o-mini, Groq Llama 3.3 70B)
- **Embeddings**: FastEmbed (integrado en Qdrant)
- **TTS**: Piper (es_ES-carlfm-x_low, 28MB)
- **STT**: Whisper (OpenAI)
- **Deployment**: Docker + GitHub Actions

### Arquitectura de Sources: Manual Source

**Concepto clave**: Cada company tiene una **source "Manual"** con un dise√±o especial:

```
source.id = company.id  // üîë KEY INSIGHT
```

**Prop√≥sito**:
- Unifica todo contenido manual de la company:
  - POST /context-units (texto manual)
  - POST /context-units/from-url (scraping manual)  
  - Emails procesados
  - Archivos subidos
  
**Creaci√≥n**:
1. ‚úÖ CLI onboarding - M√©todo principal (`python cli.py create-company`)
2. ‚úÖ Migraci√≥n SQL - Backfill para companies existentes

**Ventajas**:
- No requiere b√∫squedas (solo usar `company_id`)
- 1 source por company (predecible)
- Simplifica l√≥gica de endpoints

**Ver**: `sql/migrations/002_create_manual_sources.sql`

---

## Autenticaci√≥n

### API Key Authentication

Todos los endpoints requieren autenticaci√≥n mediante **X-API-Key** en el header:

```bash
curl -X POST https://api.semantika.es/search \
  -H "X-API-Key: sk-xxxxxxxxxxxxxxxxxxxxx" \
  -H "Content-Type: application/json" \
  -d '{"query": "noticias sobre tecnolog√≠a"}'
```

### Obtener tu API Key

1. **Via CLI** (administradores):
   ```bash
   docker exec -it semantika-api python cli.py add-client --name "Mi Cliente" --email "cliente@example.com"
   ```

2. **Via API** (no implementado todav√≠a - requiere super-admin token)

### Endpoints P√∫blicos (sin autenticaci√≥n)

- `GET /health` - Health check del sistema
- `GET /` - Informaci√≥n b√°sica del API
- `GET /docs` - Documentaci√≥n Swagger interactiva
- `GET /redoc` - Documentaci√≥n ReDoc

### Verificar tu Cliente Actual

```bash
GET /me
```

**Respuesta**:
```json
{
  "client_id": "123e4567-e89b-12d3-a456-426614174000",
  "client_name": "Mi Cliente",
  "is_active": true,
  "created_at": "2025-11-01T10:00:00Z"
}
```

---

## Gesti√≥n de Usuarios y Clientes

### CLI de Administraci√≥n

El sistema incluye un CLI completo para administraci√≥n. Ubicaci√≥n: `cli.py`

#### üè¢ Onboarding de Company (Recomendado para admins)

**Crear company completa** con un solo comando:

```bash
python cli.py create-company \
  --name "Acme Corp" \
  --cif "B12345678" \
  --tier "pro"
```

**Qu√© crea autom√°ticamente:**
1. ‚úÖ Company record en BD
2. ‚úÖ Client con API key (para integraci√≥n API)
3. ‚úÖ Source "Manual" (source.id = company.id) 
4. ‚úÖ Organization por defecto

**Output:**
```
üéâ Company Onboarding Complete!
============================================================

üìã Company Details:
   ID: 00000000-0000-0000-0000-000000000001
   Name: Acme Corp
   CIF: B12345678
   Tier: pro

üîë API Credentials:
   Client ID: abc-123-def-456
   API Key: sk-xxxxxxxxxxxxxxxxxxxxx
   ‚ö†Ô∏è  SAVE THIS KEY - won't be shown again!

üèóÔ∏è  Default Resources:
   Manual Source ID: 00000000-0000-0000-0000-000000000001
   Organization Slug: b12345678

üìù Next Steps:
   1. Create auth users: python cli.py create-auth-user ...
   2. Add sources: Use Supabase UI or API
   3. Share API key with client
```

#### üë§ Crear Usuarios Auth

**Despu√©s de crear la company**, crea usuarios para el frontend:

```bash
python cli.py create-auth-user \
  --email "usuario@acme.com" \
  --password "SecurePass123!" \
  --company-id "00000000-0000-0000-0000-000000000001" \
  --name "Usuario Acme"
```

**Output:**
```
üéâ User Created Successfully!
============================================================

üìã User Details:
   User ID: user-uuid-here
   Email: usuario@acme.com
   Password: SecurePass123!
   Company: Acme Corp

üìù Login Credentials (share with user):
   Email: usuario@acme.com
   Password: SecurePass123!
   URL: https://press.ekimen.ai
```

#### üìä Listar Clients (Legacy)

```bash
python cli.py list-clients
```

**Output**:
```
‚úÖ Client created successfully!
Client ID: 123e4567-e89b-12d3-a456-426614174000
Name: Nombre del Cliente
API Key: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

‚ö†Ô∏è  Save this API key - it won't be shown again!
```

#### Listar Clientes

```bash
docker exec -it semantika-api python cli.py list-clients
```

**Output**:
```
üìã 3 client(s) found:

ID                                   Name                           Active   Created
----------------------------------------------------------------------------------------------------
123e4567-e89b-12d3-a456-426614174000 Cliente A                      ‚úÖ       2025-11-01
456e7890-e89b-12d3-a456-426614174001 Cliente B                      ‚úÖ       2025-11-05
789e0123-e89b-12d3-a456-426614174002 Cliente C                      ‚ùå       2025-11-10
```

#### Modificar Cliente (via SQL directo en Supabase)

Para modificar clientes, usar SQL en Supabase:

```sql
-- Cambiar nombre
UPDATE clients
SET client_name = 'Nuevo Nombre'
WHERE client_id = '123e4567-e89b-12d3-a456-426614174000';

-- Desactivar cliente
UPDATE clients
SET is_active = false
WHERE client_id = '123e4567-e89b-12d3-a456-426614174000';

-- Regenerar API Key (requiere hash bcrypt)
UPDATE clients
SET api_key = 'sk-nuevo-key-aqui',
    api_key_hash = crypt('sk-nuevo-key-aqui', gen_salt('bf'))
WHERE client_id = '123e4567-e89b-12d3-a456-426614174000';
```

---

## Endpoints de API

### üìä Sistema y Estado

#### `GET /health`

Health check del sistema.

**Sin autenticaci√≥n requerida**

**Respuesta**:
```json
{
  "status": "healthy",
  "service": "semantika-api",
  "version": "0.1.2",
  "timestamp": "2025-11-11T10:00:00Z"
}
```

---

#### `GET /me`

Informaci√≥n del cliente autenticado.

**Headers**: `X-API-Key`

**Respuesta**:
```json
{
  "client_id": "123e4567-e89b-12d3-a456-426614174000",
  "client_name": "Mi Cliente",
  "is_active": true,
  "created_at": "2025-11-01T10:00:00Z"
}
```

---

### üì• Ingesta de Contenido

#### `POST /ingest/text`

Ingestar texto directamente al vector store.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "El contenido del documento a ingestar...",
  "title": "T√≠tulo del documento",
  "metadata": {
    "source": "manual",
    "category": "news"
  },
  "skip_guardrails": false
}
```

**Respuesta**:
```json
{
  "status": "success",
  "qdrant_ids": ["uuid-1", "uuid-2"],
  "chunks_created": 2
}
```

**Notas**:
- `skip_guardrails`: Si es `true`, omite validaci√≥n de PII y copyright
- El texto se divide en chunks autom√°ticamente
- Se realiza deduplicaci√≥n (similitud > 0.98)

---

#### `POST /ingest/url`

Ingestar contenido desde una URL.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "url": "https://example.com/noticia",
  "extract_multiple": false,
  "skip_guardrails": false
}
```

**Par√°metros**:
- `extract_multiple`: Si es `true`, extrae m√∫ltiples noticias de una p√°gina √≠ndice
- `skip_guardrails`: Omitir validaci√≥n de guardrails

**Respuesta**:
```json
{
  "status": "success",
  "context_units_created": 1,
  "context_units": [
    {
      "context_unit_id": "uuid-123",
      "title": "T√≠tulo extra√≠do",
      "summary": "Resumen del contenido..."
    }
  ]
}
```

---

### üîç B√∫squeda y Agregaci√≥n

#### `GET /search`

B√∫squeda sem√°ntica en el vector store.

**Headers**: `X-API-Key`

**Query Params**:
- `query` (string, requerido): Texto de b√∫squeda
- `limit` (int, default=5): N√∫mero de resultados
- `filters` (JSON string, opcional): Filtros adicionales

**Ejemplo**:
```bash
GET /search?query=noticias%20sobre%20IA&limit=10
```

**Respuesta**:
```json
{
  "results": [
    {
      "id": "uuid-1",
      "text": "Contenido del chunk...",
      "metadata": {
        "title": "T√≠tulo del documento",
        "source": "web"
      },
      "score": 0.92
    }
  ],
  "count": 10
}
```

---

#### `GET /aggregate`

B√∫squeda sem√°ntica + agregaci√≥n con LLM.

**Headers**: `X-API-Key`

**Query Params**:
- `query` (string, requerido): Pregunta o tema
- `limit` (int, default=10): Chunks a recuperar
- `threshold` (float, default=0.7): Umbral de similitud

**Ejemplo**:
```bash
GET /aggregate?query=¬øCu√°les%20son%20las%20√∫ltimas%20noticias%20sobre%20IA?&limit=15
```

**Respuesta**:
```json
{
  "query": "¬øCu√°les son las √∫ltimas noticias sobre IA?",
  "aggregated_response": "Bas√°ndome en los documentos encontrados, las principales noticias sobre IA son...",
  "sources": [
    {
      "id": "uuid-1",
      "title": "Avances en IA generativa",
      "score": 0.89
    }
  ],
  "count": 15
}
```

---

### üìù Context Units (Unidades de Contexto)

Las **context units** son documentos estructurados con an√°lisis sem√°ntico completo.

#### `GET /context-units`

Listar context units del cliente.

**Headers**: `X-API-Key`

**Query Params**:
- `limit` (int, default=20): Resultados por p√°gina
- `offset` (int, default=0): Paginaci√≥n

**Respuesta**:
```json
{
  "context_units": [
    {
      "context_unit_id": "uuid-123",
      "title": "T√≠tulo del documento",
      "summary": "Resumen corto...",
      "content": "Contenido completo...",
      "atomic_statements": [
        {
          "text": "La Gran Recogida se celebrar√° el 7 y 8 de noviembre",
          "type": "fact",
          "order": 1,
          "speaker": null
        },
        {
          "text": "Necesitamos un radar m√≥vil",
          "type": "quote",
          "order": 2,
          "speaker": "Asociaci√≥n vecinal"
        }
      ],
      "enriched_statements": [
        {
          "text": "5.000 voluntarios participar√°n en la Gran Recogida",
          "type": "fact",
          "order": 16,
          "speaker": null
        }
      ],
      "loaded_at": "2025-11-11T10:00:00Z",
      "metadata": {}
    }
  ],
  "total": 45
}
```

**Formato Unificado de Statements**:

Tanto `atomic_statements` como `enriched_statements` usan el mismo formato JSONB:

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `text` | string | Contenido del statement |
| `type` | string | `"fact"`, `"quote"`, `"context"` |
| `order` | number | Orden de aparici√≥n (secuencial) |
| `speaker` | string\|null | Atribuci√≥n (para quotes) |

**Atomic vs Enriched**:
- **Atomic**: Extra√≠dos del contenido original durante ingesta
- **Enriched**: A√±adidos posteriormente via `/enrich` con b√∫squeda web

---

#### `POST /context-units`

Crear context unit desde texto.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "Contenido del documento...",
  "title": "T√≠tulo opcional"
}
```

**Respuesta**:
```json
{
  "context_unit_id": "uuid-123",
  "title": "T√≠tulo generado o proporcionado",
  "summary": "Resumen generado por LLM",
  "atomic_statements": ["Statement 1", "Statement 2"],
  "loaded_at": "2025-11-11T10:00:00Z"
}
```

---

#### `POST /context-units/from-url`

Crear context unit desde URL (con workflow inteligente).

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "url": "https://prentsa.araba.eus/indice",
  "title": "T√≠tulo opcional"
}
```

**Respuesta**:
```json
{
  "status": "success",
  "context_units_created": 3,
  "context_units": [
    {
      "context_unit_id": "uuid-1",
      "title": "Noticia 1",
      "summary": "Resumen..."
    }
  ]
}
```

**Nota**: Este endpoint usa el workflow de scraping inteligente que:
1. Detecta si es p√°gina √≠ndice o noticia individual
2. Extrae m√∫ltiples noticias si es √≠ndice
3. Genera an√°lisis sem√°ntico completo

---

#### `POST /api/v1/context-units/{context_unit_id}/enrich`

Enriquecer context unit con informaci√≥n adicional usando b√∫squeda web en tiempo real.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**URL Params**: `context_unit_id` (UUID)

**Body**:
```json
{
  "enrich_type": "background"
}
```

**Tipos de enriquecimiento**:
- `"update"`: Actualizar con informaci√≥n reciente (busca novedades, desarrollos)
- `"background"`: Agregar contexto hist√≥rico (busca antecedentes, historia previa)
- `"verify"`: Verificar hechos con fuentes externas (valida vigencia)

**Respuesta**:
```json
{
  "success": true,
  "context_unit_id": "uuid-123",
  "context_unit_title": "T√≠tulo del Context Unit",
  "enrich_type": "background",
  "age_days": 5,
  "result": {
    "background_facts": ["antecedente1", "antecedente2"],
    "historical_context": "explicaci√≥n breve del contexto",
    "sources": ["url1", "url2"],
    "suggestion": "c√≥mo a√±adir contexto al art√≠culo"
  },
  "timestamp": "2025-11-11T10:00:00Z"
}
```

**Importante**: Los enriched statements se guardan autom√°ticamente en la BD en formato JSONB:
```json
{
  "text": "El statement enriquecido",
  "type": "fact",
  "order": 16,
  "speaker": null
}
```

Los `order` se calculan autom√°ticamente despu√©s del √∫ltimo `atomic_statement`.

**Notas**:
- Usa Groq Compound con web search autom√°tica
- Los statements enriquecidos se **acumulan** (no reemplazan los anteriores)
- Se factura como operaci√≥n "simple" (microedici√≥n)
- Compatible con formato legacy (migraci√≥n autom√°tica)

---

### ‚öôÔ∏è Procesamiento Stateless

Endpoints para procesamiento sin persistir en BD.

#### `POST /process/analyze`

Analizar texto y extraer informaci√≥n estructurada.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "Texto a analizar...",
  "action": "extract_entities",
  "params": {
    "entity_types": ["person", "organization", "location"]
  }
}
```

**Respuesta**:
```json
{
  "analysis": {
    "entities": [
      {"text": "Madrid", "type": "location"},
      {"text": "Apple", "type": "organization"}
    ]
  }
}
```

---

#### `POST /process/analyze-atomic`

Extraer atomic statements (afirmaciones at√≥micas).

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "El art√≠culo sobre IA...",
  "action": "atomic",
  "params": {}
}
```

**Respuesta**:
```json
{
  "atomic_statements": [
    "La IA generativa ha revolucionado el sector",
    "OpenAI lanz√≥ GPT-4 en marzo de 2023"
  ]
}
```

---

#### `POST /process/redact-news`

Redactar noticia desde context units (formato simple).

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "[ID:uuid-1] [ID:uuid-2]",
  "action": "redact",
  "params": {
    "style": "period√≠stico",
    "length": "medium"
  }
}
```

**Respuesta**:
```json
{
  "draft": "Noticia redactada bas√°ndose en los context units...",
  "word_count": 450
}
```

---

#### `POST /process/redact-news-rich`

Redactar noticia enriquecida con metadata (formato Kazet).

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "context_unit_ids": ["uuid-1", "uuid-2", "uuid-3"],
  "title": "T√≠tulo sugerido (opcional)",
  "instructions": "Enf√≥cate en el aspecto econ√≥mico",
  "style_guide": "Estilo formal, evitar sensacionalismo"
}
```

**Respuesta**:
```json
{
  "draft_id": "draft-uuid",
  "title": "T√≠tulo generado",
  "draft": "Contenido completo de la noticia...",
  "metadata": {
    "word_count": 520,
    "sources_used": 3,
    "model": "claude-3.5-sonnet",
    "created_at": "2025-11-11T10:00:00Z"
  }
}
```

---

#### `POST /process/micro-edit`

Micro-edici√≥n de texto con comandos simples.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "El texto a editar...",
  "command": "hacer m√°s conciso",
  "context": "Es un p√°rrafo introductorio",
  "params": {
    "max_length": 100
  }
}
```

**Comandos disponibles**:
- `"hacer m√°s conciso"`
- `"expandir"`
- `"cambiar tono a formal"`
- `"cambiar tono a informal"`
- `"corregir gram√°tica"`
- `"simplificar"`

**Respuesta**:
```json
{
  "edited_text": "Texto editado seg√∫n el comando...",
  "changes_made": ["Reducido 30%", "Eliminadas redundancias"],
  "usage_type": "simple"
}
```

**Nota**: Se factura como operaci√≥n "simple" (microedici√≥n).

---

#### `POST /process/url`

Procesar URL sin persistir.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "url": "https://example.com/article",
  "action": "extract",
  "params": {
    "extract_images": true
  }
}
```

**Respuesta**:
```json
{
  "title": "T√≠tulo extra√≠do",
  "content": "Contenido del art√≠culo...",
  "metadata": {
    "author": "Nombre Autor",
    "publish_date": "2025-11-10"
  }
}
```

---

#### `POST /styles/generate`

Generar gu√≠a de estilo basada en ejemplos.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "style_name": "Estilo Editorial X",
  "urls": [
    "https://example.com/article1",
    "https://example.com/article2",
    "https://example.com/article3"
  ]
}
```

**Respuesta**:
```json
{
  "style_guide": {
    "name": "Estilo Editorial X",
    "tone": "formal, objetivo",
    "structure": "pir√°mide invertida",
    "language": {
      "vocabulary": "t√©cnico pero accesible",
      "sentence_length": "media (15-20 palabras)"
    },
    "examples": [
      "Ejemplo de p√°rrafo tipo..."
    ]
  }
}
```

---

### üóìÔ∏è Tareas y Workflows

#### `GET /tasks`

Listar tareas del cliente.

**Headers**: `X-API-Key`

**Respuesta**:
```json
{
  "tasks": [
    {
      "task_id": "uuid-task-1",
      "source_type": "web_llm",
      "target": "https://example.com",
      "frequency_min": 60,
      "is_active": true,
      "last_run": "2025-11-11T09:00:00Z"
    }
  ]
}
```

---

#### `POST /tasks`

Crear nueva tarea programada.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "source_type": "web_llm",
  "target": "https://prentsa.araba.eus/indice",
  "frequency_min": 60,
  "config": {
    "extract_multiple": true,
    "notification_email": "alerts@example.com"
  }
}
```

**Tipos de source_type**:
- `"web_llm"`: Scraping web con LLM
- `"twitter"`: Twitter scraping
- `"api"`: Conectores API externos
- `"email"`: Monitor de correo
- `"file"`: Monitor de archivos

**Respuesta**:
```json
{
  "task_id": "uuid-task-new",
  "status": "created",
  "next_run": "2025-11-11T11:00:00Z"
}
```

---

#### `PUT /sources/{source_id}`

Actualizar configuraci√≥n de tarea (source).

**Headers**: `X-API-Key`, `Content-Type: application/json`

**URL Params**: `source_id` (UUID)

**Body**:
```json
{
  "is_active": true,
  "schedule_config": {
    "cron": "09:00"
  },
  "config": {
    "notification_enabled": true
  }
}
```

**schedule_config opciones**:
```json
// Intervalo en minutos
{"interval_minutes": 60}

// Cron diario (hora UTC)
{"cron": "09:00"}

// Cron con d√≠a de semana
{"cron": "MON,WED,FRI 14:30"}
```

**Respuesta**:
```json
{
  "source_id": "uuid-source-1",
  "status": "updated",
  "next_run": "2025-11-12T09:00:00Z"
}
```

---

#### `DELETE /tasks/{task_id}`

Eliminar tarea.

**Headers**: `X-API-Key`

**URL Params**: `task_id` (UUID)

**Respuesta**:
```json
{
  "status": "deleted",
  "task_id": "uuid-task-1"
}
```

---

#### `GET /executions`

Ver historial de ejecuciones.

**Headers**: `X-API-Key`

**Query Params**:
- `limit` (int, default=50)
- `offset` (int, default=0)
- `task_id` (UUID, opcional): Filtrar por tarea

**Respuesta**:
```json
{
  "executions": [
    {
      "execution_id": "uuid-exec-1",
      "task_id": "uuid-task-1",
      "status": "completed",
      "started_at": "2025-11-11T10:00:00Z",
      "completed_at": "2025-11-11T10:02:15Z",
      "result": {
        "context_units_created": 3,
        "errors": []
      }
    }
  ],
  "total": 127
}
```

---

### üé§ Text-to-Speech (TTS)

#### `GET /tts/health`

Health check del servicio TTS.

**Headers**: `X-API-Key`

**Respuesta**:
```json
{
  "status": "ok",
  "service": "semantika-tts",
  "version": "1.0.0",
  "model": "es_ES-carlfm-x_low",
  "quality": "x_low (3-4x faster, 28MB)",
  "integrated": true,
  "client_id": "uuid-client"
}
```

---

#### `POST /tts/synthesize`

Sintetizar voz desde texto.

**Headers**: `X-API-Key`, `Content-Type: application/json`

**Body**:
```json
{
  "text": "El texto a convertir en voz. M√°ximo 3000 caracteres.",
  "rate": 1.3
}
```

**Par√°metros**:
- `text`: Texto a sintetizar (1-3000 chars)
- `rate`: Velocidad de habla (0.5-2.0)
  - `0.5`: 50% m√°s lento
  - `1.0`: Velocidad normal
  - `1.3`: 30% m√°s r√°pido (default)
  - `2.0`: 2x m√°s r√°pido

**Respuesta**: Audio WAV stream

**Headers de respuesta**:
```
Content-Type: audio/wav
Content-Disposition: attachment; filename=speech.wav
Content-Length: [bytes]
Cache-Control: public, max-age=3600
```

**Ejemplo con curl**:
```bash
curl -X POST https://api.semantika.es/tts/synthesize \
  -H "X-API-Key: sk-xxxxx" \
  -H "Content-Type: application/json" \
  -d '{"text": "Hola, este es un test de s√≠ntesis de voz.", "rate": 1.3}' \
  --output speech.wav
```

**Ejemplo con JavaScript (chunks)**:
```javascript
async function synthesizeInChunks(text, apiKey) {
  const chunks = splitTextIntoChunks(text, 800);

  for (const chunk of chunks) {
    const response = await fetch('https://api.semantika.es/tts/synthesize', {
      method: 'POST',
      headers: {
        'X-API-Key': apiKey,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ text: chunk, rate: 1.3 })
    });

    const audioBlob = await response.blob();
    await playAudio(audioBlob);
  }
}
```

**Rendimiento esperado**:
- Chunks de 500-800 chars: ~3-4 segundos
- Textos de 2000 chars: ~8-10 segundos
- Textos de 3000 chars: ~12-15 segundos

**Notas**:
- Timeout: 15 segundos
- Modelo: Piper es_ES-carlfm-x_low (voz masculina espa√±ola)
- Se factura como operaci√≥n "simple" (microedici√≥n)
- Recomendado: Dividir textos largos en chunks de 800 chars

---

### üéôÔ∏è Speech-to-Text (STT)

**Nota**: El servicio STT usa Whisper de OpenAI pero no est√° expuesto como endpoint p√∫blico.

Para usar STT:

1. **Via audio_transcriber.py** (interno):
   ```python
   from sources.audio_transcriber import AudioTranscriber

   transcriber = AudioTranscriber()
   result = await transcriber.transcribe_audio("path/to/audio.mp3")
   ```

2. **Modelos disponibles**:
   - `base`: R√°pido, menos preciso
   - `small`: Balance velocidad/precisi√≥n (default)
   - `medium`: M√°s preciso, m√°s lento

---

## Servicios de Procesamiento

### üß† Procesamiento con LLM

El sistema usa **OpenRouter** para acceso a m√∫ltiples modelos LLM:

#### Modelos Disponibles

| Modelo | Uso | Velocidad | Costo |
|--------|-----|-----------|-------|
| `anthropic/claude-3.5-sonnet` | An√°lisis complejo, redacci√≥n | Media | Alto |
| `openai/gpt-4o-mini` | Tareas r√°pidas, extracciones | R√°pida | Bajo |
| `groq/llama-3.3-70b` | Scraping, an√°lisis web | Muy r√°pida | Medio |

#### Operaciones LLM

**An√°lisis Sem√°ntico**:
- Extracci√≥n de entidades
- Generaci√≥n de res√∫menes
- Atomic statements
- Clasificaci√≥n de contenido

**Redacci√≥n**:
- Noticias desde context units
- Micro-ediciones
- Expansi√≥n/condensaci√≥n de texto

**Scraping Inteligente**:
- Detecci√≥n de estructura de p√°gina
- Extracci√≥n de m√∫ltiples art√≠culos
- Limpieza y normalizaci√≥n de HTML

---

### üîç B√∫squeda Sem√°ntica (Qdrant)

Vector store para b√∫squedas sem√°nticas multi-tenant.

#### Caracter√≠sticas

- **Embeddings**: FastEmbed (integrado en Qdrant)
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimensiones**: 384
- **Similitud**: Cosine similarity

#### Filtrado Multi-tenant

Todos los queries incluyen filtro autom√°tico por `client_id`:

```python
search_results = qdrant.search(
    collection_name="semantika_prod",
    query_vector=embedding,
    query_filter={
        "must": [
            {"key": "client_id", "match": {"value": client_id}}
        ]
    },
    limit=10
)
```

#### Deduplicaci√≥n

Antes de insertar nuevo contenido:
1. Calcular embedding del t√≠tulo o primeros 200 chars
2. Buscar similitud > 0.98 en Qdrant
3. Si existe duplicado ‚Üí Descartar y loguear
4. Si no existe ‚Üí Insertar

---

### üõ°Ô∏è Guardrails

Sistema de validaci√≥n antes de ingesta.

#### 1. Detecci√≥n de PII

Detecta y anonimiza informaci√≥n personal:
- Nombres completos
- DNI/NIE
- N√∫meros de tel√©fono
- Emails
- Direcciones

**Ejemplo**:
```
Input: "Juan P√©rez (DNI 12345678X) llam√≥ al 600123456"
Output: "[NOMBRE] ([DNI]) llam√≥ al [TEL√âFONO]"
```

#### 2. Copyright Detection

Detecta contenido con copyright:
- ¬© symbols
- "All rights reserved"
- "Prohibida reproducci√≥n"

Si detectado ‚Üí Rechazar ingesta

#### 3. Robots.txt Compliance

Antes de scrapear:
1. Fetch robots.txt del dominio
2. Verificar si la ruta est√° permitida
3. Si prohibido ‚Üí Bloquear scraping

**Bypass**: `skip_guardrails: true` (solo para administradores)

---

## Workflows y Tareas Programadas

### Scheduler (APScheduler)

El componente `semantika-scheduler` ejecuta tareas programadas usando **APScheduler**.

#### Tipos de Triggers

**1. IntervalTrigger** (cada X minutos):
```json
{
  "frequency_min": 60
}
```

**2. CronTrigger** (hora espec√≠fica):
```json
{
  "schedule_config": {
    "cron": "09:00"
  }
}
```

**3. Cron con d√≠as de semana**:
```json
{
  "schedule_config": {
    "cron": "MON,WED,FRI 14:30"
  }
}
```

#### Flujo de Ejecuci√≥n

```
1. Scheduler carga tareas activas desde Supabase
2. Crea APScheduler jobs
3. En cada ejecuci√≥n:
   ‚îú‚îÄ Marca execution como "running"
   ‚îú‚îÄ Ejecuta source connector
   ‚îú‚îÄ Procesa resultados
   ‚îú‚îÄ Marca execution como "completed" o "failed"
   ‚îî‚îÄ Loguea resultado
4. Recarga configuraci√≥n cada 5 minutos
```

#### Reload Din√°mico

El scheduler recarga las tareas cada 5 minutos para capturar cambios en configuraci√≥n sin reiniciar el servicio.

**Nota importante**: Solo actualiza jobs si detecta cambios reales (frecuencia, activaci√≥n/desactivaci√≥n) para no resetear timers.

---

### Source Connectors

Conectores para diferentes fuentes de datos.

#### 1. Web Scraper (web_llm)

**Archivo**: `sources/scraper_workflow.py`

**Caracter√≠sticas**:
- Usa LangGraph workflow con Groq Llama 3.3 70B
- Detecta autom√°ticamente:
  - P√°gina √≠ndice ‚Üí Extrae m√∫ltiples art√≠culos
  - Noticia individual ‚Üí Extrae contenido
- Genera atomic statements
- Crea context units completas

**Configuraci√≥n**:
```json
{
  "source_type": "web_llm",
  "target": "https://prentsa.araba.eus/indice",
  "schedule_config": {"cron": "09:00"},
  "config": {
    "extract_multiple": true,
    "max_articles": 10
  }
}
```

---

#### 2. Twitter Scraper

**Archivo**: `sources/twitter_scraper.py`

**Caracter√≠sticas**:
- Usa ScraperTech API
- Extrae tweets por usuario o hashtag
- Filtra por fecha

**Configuraci√≥n**:
```json
{
  "source_type": "twitter",
  "target": "@username",
  "frequency_min": 120,
  "config": {
    "max_tweets": 50,
    "include_replies": false
  }
}
```

---

#### 3. API Connectors

**Archivo**: `sources/api_connectors.py`

Conectores para:
- **Agencia EFE** (noticias)
- **Reuters** (noticias)
- **WordPress** (blogs)

**Configuraci√≥n EFE**:
```json
{
  "source_type": "api",
  "target": "efe",
  "frequency_min": 60,
  "config": {
    "api_key": "tu-clave-efe",
    "category": "tecnologia"
  }
}
```

---

#### 4. Perplexity News Connector

**Archivo**: `sources/perplexity_news_connector.py`

Usa Perplexity API para buscar noticias recientes sobre un tema.

**Configuraci√≥n**:
```json
{
  "source_type": "perplexity",
  "target": "inteligencia artificial Espa√±a",
  "frequency_min": 180,
  "config": {
    "max_results": 10,
    "recency_days": 7
  }
}
```

---

## Monitores Autom√°ticos

### Email Monitor

**Archivo**: `sources/email_monitor.py`

Monitorea buz√≥n IMAP y extrae contenido de emails.

#### Configuraci√≥n (.env)

```bash
EMAIL_MONITOR_ENABLED=true
EMAIL_IMAP_SERVER=imap.gmail.com
EMAIL_IMAP_PORT=993
EMAIL_ADDRESS=tu-email@gmail.com
EMAIL_PASSWORD=app-password-aqui
EMAIL_MONITOR_INTERVAL=60
```

#### Gmail Setup

1. Activar 2FA en tu cuenta Google
2. Generar "App Password" en https://myaccount.google.com/apppasswords
3. Usar ese password en `EMAIL_PASSWORD`

#### Funcionamiento

```
1. Conecta a IMAP cada X minutos
2. Busca emails no le√≠dos
3. Extrae:
   ‚îú‚îÄ Subject ‚Üí title
   ‚îú‚îÄ Body (text/html) ‚Üí content
   ‚îî‚îÄ Attachments (PDF, DOCX) ‚Üí extraer texto
4. Crea context unit
5. Marca email como le√≠do
```

#### Multi-empresa

**Archivo**: `sources/multi_company_email_monitor.py`

Permite monitorear m√∫ltiples cuentas de email (una por cliente).

**Configuraci√≥n en Supabase**:
```sql
INSERT INTO email_accounts (client_id, email_address, imap_server, imap_port, password_encrypted)
VALUES ('uuid-client', 'cliente@example.com', 'imap.gmail.com', 993, encrypt('password'));
```

---

### File Monitor

**Archivo**: `sources/file_monitor.py`

Monitorea directorio y procesa archivos nuevos.

#### Configuraci√≥n (.env)

```bash
FILE_MONITOR_ENABLED=true
FILE_MONITOR_WATCH_DIR=/app/data/watch
FILE_MONITOR_PROCESSED_DIR=/app/data/processed
FILE_MONITOR_INTERVAL=30
```

#### Formatos Soportados

- **Texto**: `.txt`, `.md`
- **Documentos**: `.pdf`, `.docx`, `.odt`
- **Web**: `.html`, `.htm`

#### Funcionamiento

```
1. Escanea FILE_MONITOR_WATCH_DIR cada X segundos
2. Para cada archivo nuevo:
   ‚îú‚îÄ Extrae texto seg√∫n formato
   ‚îú‚îÄ Crea context unit
   ‚îî‚îÄ Mueve a FILE_MONITOR_PROCESSED_DIR
3. Loguea resultado
```

---

## Crear Workflows Personalizados

### Estructura de un Source Connector

Todos los conectores heredan de `BaseSource`:

```python
# sources/mi_conector.py

from sources.base_source import BaseSource
from utils.logger import get_logger

logger = get_logger("mi_conector")

class MiConector(BaseSource):
    """Descripci√≥n del conector."""

    async def fetch_data(self) -> List[Dict[str, Any]]:
        """
        Obtener datos de la fuente externa.

        Returns:
            Lista de documentos con formato:
            [
                {
                    "title": "T√≠tulo",
                    "content": "Contenido",
                    "metadata": {"source": "mi_api"}
                }
            ]
        """
        logger.info("mi_conector_fetch", target=self.target)

        # Tu l√≥gica aqu√≠
        data = await self._call_external_api()

        return [
            {
                "title": item["name"],
                "content": item["description"],
                "metadata": {
                    "source": "mi_api",
                    "id": item["id"]
                }
            }
            for item in data
        ]

    async def _call_external_api(self):
        """L√≥gica espec√≠fica de tu API."""
        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.get(
                "https://api.example.com/data",
                headers={"Authorization": f"Bearer {self.config.get('api_key')}"}
            ) as response:
                return await response.json()
```

---

### Registrar el Conector

1. **Importar en scheduler.py**:

```python
# scheduler.py

from sources.mi_conector import MiConector

# En la funci√≥n get_source_connector():
def get_source_connector(source):
    source_type = source["source_type"]

    if source_type == "mi_conector":
        return MiConector(
            source_id=source["source_id"],
            client_id=source["client_id"],
            target=source["target"],
            config=source.get("config", {})
        )
    # ... otros conectores
```

2. **Crear tarea con el nuevo tipo**:

```bash
docker exec -it semantika-api python cli.py add-task \
  --client-id "uuid-cliente" \
  --type "mi_conector" \
  --target "https://api.example.com" \
  --freq 120
```

O via API:
```bash
POST /tasks
{
  "source_type": "mi_conector",
  "target": "https://api.example.com",
  "frequency_min": 120,
  "config": {
    "api_key": "mi-clave-api"
  }
}
```

---

### Librer√≠as Comunes para Workflows

#### 1. HTTP Requests

```python
import aiohttp

async with aiohttp.ClientSession() as session:
    async with session.get(url) as response:
        data = await response.json()
```

#### 2. HTML Parsing

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')
title = soup.find('h1').text
content = soup.find('article').get_text()
```

#### 3. LLM Processing

```python
from utils.llm_client import get_llm_client

llm_client = get_llm_client()

result = await llm_client.extract_atomic_statements(
    text=content,
    client_id=client_id
)
```

#### 4. Supabase

```python
from utils.supabase_client import get_supabase_client

supabase = get_supabase_client()

await supabase.create_context_unit(
    client_id=client_id,
    title=title,
    content=content,
    atomic_statements=statements
)
```

#### 5. Qdrant

```python
from utils.qdrant_client import get_qdrant_client

qdrant = get_qdrant_client()

await qdrant.upsert(
    collection_name="semantika_prod",
    points=[
        {
            "id": str(uuid.uuid4()),
            "vector": embedding,
            "payload": {
                "client_id": client_id,
                "text": content,
                "metadata": metadata
            }
        }
    ]
)
```

---

### Ejemplos de Workflows

#### Ejemplo 1: RSS Feed Connector

```python
# sources/rss_connector.py

import feedparser
from sources.base_source import BaseSource

class RSSConnector(BaseSource):
    """Conector para feeds RSS."""

    async def fetch_data(self):
        feed = feedparser.parse(self.target)

        return [
            {
                "title": entry.title,
                "content": entry.description,
                "metadata": {
                    "source": "rss",
                    "published": entry.published,
                    "link": entry.link
                }
            }
            for entry in feed.entries[:10]
        ]
```

#### Ejemplo 2: GitHub Issues Monitor

```python
# sources/github_issues.py

import aiohttp
from sources.base_source import BaseSource

class GitHubIssuesMonitor(BaseSource):
    """Monitor de issues de GitHub."""

    async def fetch_data(self):
        # self.target = "owner/repo"
        owner, repo = self.target.split('/')

        url = f"https://api.github.com/repos/{owner}/{repo}/issues"

        async with aiohttp.ClientSession() as session:
            async with session.get(
                url,
                headers={"Authorization": f"token {self.config['github_token']}"}
            ) as response:
                issues = await response.json()

        return [
            {
                "title": f"Issue #{issue['number']}: {issue['title']}",
                "content": issue['body'] or "",
                "metadata": {
                    "source": "github",
                    "issue_number": issue['number'],
                    "state": issue['state'],
                    "url": issue['html_url']
                }
            }
            for issue in issues
            if issue['state'] == 'open'
        ]
```

---

## Configuraci√≥n del Sistema

### Variables de Entorno

Archivo `.env` en la ra√≠z del proyecto:

```bash
# Supabase (Base de datos de configuraci√≥n)
SUPABASE_URL=https://tu-proyecto.supabase.co
SUPABASE_KEY=tu-supabase-service-role-key

# Qdrant (Vector store)
QDRANT_URL=https://cluster.cloud.qdrant.io:6333
QDRANT_API_KEY=tu-qdrant-api-key
QDRANT_COLLECTION_NAME=semantika_prod

# OpenRouter (LLMs)
OPENROUTER_API_KEY=sk-or-v1-tu-clave
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_DEFAULT_MODEL=anthropic/claude-3.5-sonnet
OPENROUTER_FAST_MODEL=openai/gpt-4o-mini

# Groq (Scraping r√°pido)
GROQ_API_KEY=tu-groq-api-key

# ScraperTech (Twitter)
SCRAPERTECH_API_KEY=tu-scrapertech-key
SCRAPERTECH_BASE_URL=https://api.scraper.tech

# Perplexity (Noticias)
PERPLEXITY_API_KEY=pplx-tu-clave

# Procesamiento de texto
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
SIMILARITY_THRESHOLD=0.98

# TTL (d√≠as antes de borrar datos no especiales)
DATA_TTL_DAYS=30

# Servidor
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO

# File Monitor
FILE_MONITOR_ENABLED=false
FILE_MONITOR_WATCH_DIR=/app/data/watch
FILE_MONITOR_PROCESSED_DIR=/app/data/processed
FILE_MONITOR_INTERVAL=30

# Email Monitor
EMAIL_MONITOR_ENABLED=false
EMAIL_IMAP_SERVER=imap.gmail.com
EMAIL_IMAP_PORT=993
EMAIL_ADDRESS=tu-email@example.com
EMAIL_PASSWORD=tu-app-password
EMAIL_MONITOR_INTERVAL=60
```

---

### Docker Compose

```yaml
version: '3.8'

services:
  semantika-api:
    build: .
    container_name: semantika-api
    command: "uvicorn server:app --host 0.0.0.0 --port 8000 --reload"
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - .:/app  # Hot-reload en desarrollo
    restart: unless-stopped
    networks:
      - semantika-network

  semantika-scheduler:
    build: .
    container_name: semantika-scheduler
    command: "python scheduler.py"
    env_file:
      - .env
    volumes:
      - .:/app
    restart: unless-stopped
    networks:
      - semantika-network

networks:
  semantika-network:
    driver: bridge
```

---

### Deployment (Easypanel)

El sistema se despliega autom√°ticamente via GitHub Actions cuando se hace push a `main`.

**Workflow**:
1. Push a GitHub ‚Üí Trigger GitHub Action
2. GitHub Action ‚Üí SSH a servidor
3. Servidor ejecuta:
   ```bash
   cd /path/to/semantika
   git pull
   docker-compose up -d --build
   ```
4. Easypanel detecta cambios y reconstruye contenedores

**Tiempo de rebuild**: ~10 minutos (por descarga de modelo Piper TTS)

---

## Uso y Facturaci√≥n

### Tipos de Operaciones

El sistema trackea uso para facturaci√≥n:

| Tipo | Descripci√≥n | Coste Relativo |
|------|-------------|----------------|
| `simple` | Microediciones, TTS | 1x |
| `standard` | B√∫squedas, an√°lisis b√°sico | 5x |
| `complex` | Redacci√≥n completa, workflows LLM | 20x |

### Tracking de Uso

Tabla `usage_logs` en Supabase:

```sql
CREATE TABLE usage_logs (
  usage_id UUID PRIMARY KEY,
  organization_id UUID,
  client_id UUID,
  model VARCHAR(100),
  operation VARCHAR(100),
  input_tokens INT,
  output_tokens INT,
  metadata JSONB,
  created_at TIMESTAMP
);
```

### Consultar Uso

**Via SQL en Supabase**:
```sql
-- Uso por cliente en el √∫ltimo mes
SELECT
  client_id,
  COUNT(*) as total_operations,
  SUM(input_tokens) as total_input_tokens,
  SUM(output_tokens) as total_output_tokens
FROM usage_logs
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY client_id;

-- Desglose por tipo de operaci√≥n
SELECT
  operation,
  metadata->>'usage_type' as usage_type,
  COUNT(*) as count,
  AVG(input_tokens + output_tokens) as avg_tokens
FROM usage_logs
WHERE client_id = 'uuid-cliente'
  AND created_at > NOW() - INTERVAL '30 days'
GROUP BY operation, usage_type;
```

**Via API** (futuro):
```bash
GET /usage/report?start_date=2025-11-01&end_date=2025-11-30
```

---

## L√≠mites y Consideraciones

### Rate Limits

- **TTS**: 15 segundos timeout por request
- **LLM (Groq)**: 12,000 tokens por request
- **B√∫squeda Qdrant**: 100 resultados m√°ximo por query

### Tama√±o de Datos

- **Ingesta de texto**: 50,000 caracteres m√°ximo
- **TTS**: 3,000 caracteres m√°ximo (recomendado: chunks de 800)
- **Context units**: Sin l√≠mite (pero se aplica TTL de 30 d√≠as si `special_info=false`)

### TTL (Time to Live)

Datos con `special_info=false` se borran autom√°ticamente despu√©s de 30 d√≠as (configurable con `DATA_TTL_DAYS`).

Para marcar datos como especiales:
```sql
UPDATE context_units
SET special_info = true
WHERE context_unit_id = 'uuid-importante';
```

---

## Troubleshooting

### Logs

**Ver logs del API**:
```bash
docker logs -f semantika-api
```

**Ver logs del scheduler**:
```bash
docker logs -f semantika-scheduler
```

**Formato de logs** (JSON):
```json
{
  "level": "INFO",
  "timestamp": "2025-11-11T10:00:00.123Z",
  "service": "api",
  "action": "search_completed",
  "client_id": "uuid-123",
  "duration_ms": 234.5
}
```

### Errores Comunes

#### 401 Unauthorized
**Causa**: API Key inv√°lida o faltante
**Soluci√≥n**: Verificar header `X-API-Key`

#### 403 Forbidden
**Causa**: API Key v√°lida pero cliente inactivo
**Soluci√≥n**: Activar cliente en Supabase

#### 429 Rate Limit
**Causa**: Demasiadas requests (rate limit de OpenRouter/Groq)
**Soluci√≥n**: Esperar 1 minuto, reducir frecuencia de tareas

#### 500 Internal Server Error
**Causa**: Error en procesamiento (LLM, Qdrant, Supabase)
**Soluci√≥n**: Revisar logs para detalles espec√≠ficos

### Health Checks

```bash
# API health
curl https://api.semantika.es/health

# TTS health (requiere API key)
curl https://api.semantika.es/tts/health \
  -H "X-API-Key: sk-xxxxx"

# Qdrant health (directo)
curl https://cluster.cloud.qdrant.io:6333/health
```

---

## Soporte y Contacto

- **Documentaci√≥n**: Este archivo + `/docs` en API
- **Logs**: Ver secci√≥n Troubleshooting
- **Issues**: GitHub Issues (repositorio privado)

---

## Changelog

### v0.1.2 (2025-11-11)
- ‚úÖ A√±adido servicio TTS con Piper (modelo x_low)
- ‚úÖ Workflow de scraping inteligente con Groq
- ‚úÖ Fix scheduler: no resetear timers innecesariamente
- ‚úÖ Context units enriquecidas con atomic statements
- ‚úÖ Tracking de uso mejorado (simple/standard/complex)

### v0.1.1 (2025-11-08)
- ‚úÖ Email monitor multi-empresa
- ‚úÖ Perplexity news connector
- ‚úÖ Micro-ediciones con comandos simples
- ‚úÖ Generaci√≥n de gu√≠as de estilo

### v0.1.0 (2025-11-01)
- ‚úÖ Release inicial
- ‚úÖ API REST completo
- ‚úÖ Scheduler con APScheduler
- ‚úÖ Web scraper, Twitter, API connectors
- ‚úÖ Qdrant + Supabase integration

---

**Fin de la documentaci√≥n**
