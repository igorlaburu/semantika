# GuÃ­a de Desarrollo para Claude Code

Este documento contiene instrucciones especÃ­ficas para Claude Code al trabajar en el proyecto **semantika**.

## Contexto del Proyecto

`semantika` es un pipeline de datos semÃ¡nticos multi-tenant diseÃ±ado para operar como servicio headless. Agrega, procesa y unifica informaciÃ³n de mÃºltiples fuentes en PostgreSQL/pgvector para bÃºsquedas semÃ¡nticas hÃ­bridas, alertas y agregaciÃ³n.

### TerminologÃ­a clave

- **UC (Unidad de Contexto)**: Set de informaciÃ³n de uno o varios statements que se obtiene de diversas fuentes primarias o de forma manual del usuario, email, texto aÃ±adido o micrÃ³fono.
- **ArtÃ­culo**: Producto de la generaciÃ³n de nuestro sistema listo para publicar a partir de una o varias UC.

## Stack TecnolÃ³gico

- **Backend**: Python 3.10+, FastAPI, APScheduler
- **Base de Datos**: Supabase PostgreSQL + pgvector (embeddings 768d)
- **LLM**: OpenRouter (Claude 3.5 Sonnet), Groq (Llama 3.3 70B - gratis)
- **Embeddings**: âœ… **FastEmbed local** (`paraphrase-multilingual-mpnet-base-v2`, 768d)
- **BÃºsqueda**: HÃ­brida (semantic pgvector + keyword full-text)
- **OrquestaciÃ³n**: Docker Compose
- **Deployment**: GitHub Actions (CI/CD automÃ¡tico)

## Arquitectura de Datos

### Sistema Unificado PostgreSQL + pgvector

**Estado actual**: âœ… MigraciÃ³n completa de Qdrant â†’ PostgreSQL (dic 2024)

**Ventajas**:
- âœ… Una sola BD (config + vectores) - simplicidad operacional
- âœ… RLS policies para multi-tenancy seguro
- âœ… BÃºsqueda hÃ­brida (semantic + keyword en una query)
- âœ… Joins nativos (context_units + sources + companies)
- âœ… No necesita sincronizaciÃ³n entre Qdrant y Supabase

**Colecciones**:
- `press_context_units`: Noticias de prensa (company-specific + pool)
- `web_context_units`: Monitoring web (subvenciones, formularios)

**Pool compartido**:
- UUID: `99999999-9999-9999-9999-999999999999`
- Contenido pÃºblico accesible por todos los clientes
- Discovery automÃ¡tico vÃ­a GNews + LLM (cada 3 dÃ­as)
- Ingesta horaria de fuentes descubiertas

### Embeddings FastEmbed (Local)

**Modelo**: `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- Dimensiones: 768
- Idiomas: 50+ (espaÃ±ol, euskera, catalÃ¡n, gallego, inglÃ©s...)
- Velocidad: ~100-200ms por embedding (CPU)
- UbicaciÃ³n: `utils/embedding_generator.py`

**Performance**:
- Startup: Modelo precargado en `/app/startup` (~1-2 segundos)
- Inferencia: ~150ms por query en VPS modesto
- Costo: $0 (100% local)

### BÃºsqueda HÃ­brida (Semantic + Keyword)

**Endpoint**: `POST /api/v1/context-units/search-vector`

**3 tÃ©cnicas combinadas**:
1. **Query expansion**: Cache (1h) + sinÃ³nimos locales + LLM Groq (gratis)
2. **Semantic search**: pgvector cosine similarity (threshold 0.18)
3. **Keyword search**: PostgreSQL full-text search (Spanish config)

**Re-ranking**: `0.7 * semantic_score + 0.3 * keyword_score`

**Performance**:
- Latencia: 150-200ms (con cache) / 300-400ms (sin cache)
- Costo: $0 (Groq gratis para expansiÃ³n)
- Threshold: 0.18 (vs 0.25 anterior, +40% resultados)

## Arquitectura

### Componentes Docker
1. **semantika-api**: FastAPI server (puerto 8000)
2. **semantika-scheduler**: APScheduler daemon
   - **Auto-reload**: Recarga sources cada 5 minutos automÃ¡ticamente
   - **NO reiniciar** despuÃ©s de cambiar schedules en BD - esperar hasta 5min
3. **qdrant**: Vector database (puerto 6333)

### Zonas Horarias
- **EspaÃ±a**: UTC+1 (CET) en invierno, UTC+2 (CEST) en verano
- **Scheduler**: Usa **UTC** siempre
- **ConversiÃ³n**: EspaÃ±a 13:00 = UTC 12:00 (invierno)

### Flujo de Datos
1. Ingesta â†’ Guardrails (PII/Copyright) â†’ DesduplicaciÃ³n â†’ Qdrant
2. BÃºsqueda â†’ Filtrado por client_id â†’ AgregaciÃ³n (opcional con LLM)

## Reglas de Desarrollo

### 0. source_metadata Schema EstÃ¡ndar

**TODOS los conectores DEBEN usar el mismo formato de `source_metadata`**:

```python
from utils.source_metadata_schema import normalize_source_metadata

# âœ… CORRECTO: Usar schema estÃ¡ndar
metadata = normalize_source_metadata(
    url="https://www.rtve.es/noticias/...",     # URL canÃ³nica (REQUIRED para web)
    source_name="RTVE",                          # Nombre legible
    published_at="2025-12-11T17:22:50Z",        # ISO 8601 (con timezone Z)
    scraped_at="2025-12-11T17:25:00Z",          # ISO 8601 (auto-generado si None)
    connector_type="perplexity_news",            # Identificador del conector
    connector_specific={                         # Datos especÃ­ficos del conector
        "perplexity_query": "Bilbao, Bizkaia",
        "perplexity_index": 5,
        "enrichment_model": "gpt-4o-mini"
    }
)

# âŒ INCORRECTO: Inventar campos propios
metadata = {
    "perplexity_source": "https://...",  # NO - usar 'url'
    "perplexity_date": "2025-12-11",     # NO - usar 'published_at' en ISO 8601
    "perplexity_query": "Bilbao"         # NO - va en 'connector_specific'
}
```

**Campos estÃ¡ndar** (top-level):
- `url`: URL canÃ³nica (None para emails)
- `source_name`: Nombre legible de la fuente
- `published_at`: Fecha de publicaciÃ³n (ISO 8601 con timezone)
- `scraped_at`: Fecha de captura (ISO 8601, auto-generado)
- `connector_type`: `perplexity_news`, `scraping`, `email`, etc.
- `featured_image`: Dict con metadata de imagen destacada (opcional)
  - `url`: URL de la imagen
  - `source`: MÃ©todo de extracciÃ³n (`og:image`, `twitter:image`, `jsonld`, `content`)
  - `width`: Ancho en pixels (opcional)
  - `height`: Alto en pixels (opcional)
  - `alt`: Texto alternativo (opcional)
- `connector_specific`: Dict con datos especÃ­ficos del conector

**MigraciÃ³n de datos existentes**:
```bash
# Dry-run (ver cambios sin aplicar)
python migrations/migrate_source_metadata.py --dry-run --limit 10

# Ejecutar migraciÃ³n
python migrations/migrate_source_metadata.py --no-dry-run
```

### 0.1. Featured Images (ImÃ¡genes Destacadas)

**ImplementaciÃ³n**: ExtracciÃ³n automÃ¡tica de imÃ¡genes representativas de fuentes web

**CuÃ¡ndo extraer**:
- âœ… **SOLO despuÃ©s del quality gate** (atomic_statements >= 2)
- âŒ NO extraer para contenido rechazado (evita overhead)

**Cascada de extracciÃ³n** (prioridad):
1. **Open Graph** (`og:image`) - 90% de sitios, estÃ¡ndar social media
2. **Twitter Card** (`twitter:image`) - 5% adicional
3. **JSON-LD Schema.org** - Sitios tÃ©cnicos/noticias
4. **Primera imagen article** - Ãšltimo recurso

**Aspect ratio esperado**: **1.91:1** (1200Ã—630px Open Graph estÃ¡ndar)

**Endpoint de imagen**:
```bash
GET /api/v1/context-units/{id}/image
Authorization: Bearer {api_key}

# Responde:
# - Imagen original (JPEG/PNG) si existe
# - Placeholder SVG (600Ã—314px) si no existe o falla
# 
# Headers:
# - Cache-Control: public, max-age=86400 (24h)
# - X-Image-Source: "original" | "placeholder"
# - X-Image-Extraction: "og:image" | "twitter:image" | "jsonld" | "content"
```

**Display recomendado**:
```css
/* Thumbnail en detalle de noticia (NO en lista) */
.thumbnail-container {
  width: 100%;
  max-width: 600px;
  aspect-ratio: 1.91 / 1;
  overflow: hidden;
}

.thumbnail-container img {
  width: 100%;
  height: 100%;
  object-fit: cover;
}
```

**IntegraciÃ³n en conectores**:
```python
from utils.image_extractor import extract_featured_image
from utils.source_metadata_schema import normalize_source_metadata

# DespuÃ©s del quality gate (statements >= 2)
featured_image = None
if len(atomic_statements) >= 2:
    featured_image = extract_featured_image(soup, url)

# AÃ±adir a metadata
metadata = normalize_source_metadata(
    url=url,
    source_name="RTVE",
    published_at="2025-12-11T00:00:00Z",
    connector_type="scraping",
    featured_image=featured_image,  # Puede ser None
    connector_specific={}
)
```

**Performance**:
- ExtracciÃ³n: ~50-100ms (parsing HTML)
- Proxy: ~200-500ms (descarga de fuente remota)
- Caching: Browser cache (Cache-Control: 24h)
- Volumen: ~50-100 imÃ¡genes/mes

**Ventajas del proxy**:
- âœ… Oculta URL original
- âœ… Evita hotlinking blocks (User-Agent + Referer)
- âœ… Fallback a placeholder si falla
- âœ… Headers de caching correctos

### 1. Logging
- **SIEMPRE** usar JSON estructurado en stdout
- Formato: `{"level": "INFO", "timestamp": "...", "service": "...", "action": "...", "client_id": "...", ...}`
- Niveles: DEBUG, INFO, WARN, ERROR

### 2. Multi-tenancy
- **NUNCA** permitir queries sin filtro `client_id`
- Validar API Key en **cada** request
- Aislar datos estrictamente por cliente

### 3. Seguridad
- **NO** commitear `.env`
- **NO** loguear API keys o credenciales
- Anonimizar PII antes de vectorizar
- Validar robots.txt antes de scrapear

### 4. Variables de Entorno
Usar `.env` para:
- `SUPABASE_URL`, `SUPABASE_KEY`
- `OPENROUTER_API_KEY`
- `SCRAPERTECH_API_KEY`
- `QDRANT_URL`, `QDRANT_COLLECTION_NAME`
- Configuraciones de procesamiento

### 5. Dependencias
Mantener `requirements.txt` sincronizado con:
- FastAPI, uvicorn
- supabase-py, qdrant-client
- langchain, langchain-openai
- openai-whisper (opcional, pesado)
- beautifulsoup4, requests

### 6. Guardrails
Implementar **antes** de la desduplicaciÃ³n:
1. **PII Detection**: LLM few-shot â†’ Anonimizar â†’ Log WARN
2. **Copyright**: LLM pattern match â†’ Rechazar â†’ Log INFO
3. **Robots.txt**: Verificar allow/disallow â†’ Bloquear si prohibido

### 7. DesduplicaciÃ³n
- Calcular embedding del `title` o primeros 200 chars
- Buscar similitud > 0.98 en Qdrant (filtrado por `client_id`)
- Si duplicado â†’ Descartar â†’ Log DEBUG

### 8. TTL (Time-to-Live)
- Datos con `special_info=false`: Borrar despuÃ©s de 30 dÃ­as
- Job diario en `scheduler.py`
- Filtro Qdrant: `loaded_at < (now - 30 days) AND special_info = false`

## Estructura de Archivos

```
/semantika/
â”œâ”€â”€ .env                  # Secretos (NO commitear)
â”œâ”€â”€ .env.example          # Plantilla
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.md       # Arquitectura completa
â”œâ”€â”€ CLAUDE.md            # Este archivo
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ server.py            # FastAPI API
â”œâ”€â”€ scheduler.py         # APScheduler cron
â”œâ”€â”€ core_ingest.py       # Motor de ingesta
â”œâ”€â”€ cli.py               # Admin CLI
â”œâ”€â”€ /sources/            # Conectores (web, twitter, api, audio)
â”œâ”€â”€ /utils/              # Logging, helpers
â””â”€â”€ /sql/                # Scripts Supabase
```

## Tareas Pendientes

### ðŸ”„ Refactoring de server.py (PENDIENTE - Recordatorio diario)
- [ ] **TODO CRÃTICO**: Refactorizar monolito server.py (4700+ lÃ­neas) a estructura modular con FastAPI APIRouter
  - **Problema actual**: Todo estÃ¡ en un solo archivo gigante (anti-patrÃ³n de mantenimiento)
  - **Propuesta**: Empezar con `api/routes/images.py` como prueba piloto
  - **Plan**: Si funciona bien â†’ continuar con articles.py, context_units.py, publications.py, search.py, etc.
  - **Ventajas**: CÃ³digo mÃ¡s mantenible, PRs mÃ¡s pequeÃ±os, testing modular, separaciÃ³n de responsabilidades
  - **Riesgo**: Posibles dependencias implÃ­citas o imports circulares ocultos
  - **Enfoque**: Conservador - probar solo images endpoints primero, revertir rÃ¡pido si hay problemas
  - **RECORDATORIO DIARIO**: Â¿Es buen momento hoy para refactorizar images.py?

### Pool Discovery & Ingestion
- [ ] **TODO**: Mover schedules de pool discovery/ingestion a BD (tabla pool_discovery_config)
  - Actualmente hardcoded en scheduler.py (discovery cada hora :30, ingestion cada hora :00)
  - DeberÃ­a leer schedule_config desde pool_discovery_config por geographic_area
  - Permitir diferentes frecuencias por regiÃ³n (ej: Ãlava cada 2h, Bizkaia cada 6h)
- [ ] **TODO**: Mejorar validaciÃ³n de URLs en discovery_flow.py
  - AÃ±adir post-validaciÃ³n despuÃ©s de extract_index_url()
  - Detectar y rechazar URLs con IDs numÃ©ricos largos o slugs especÃ­ficos
  - Ejemplo rechazar: `/events/106714-titulo-largo` â†’ Aceptar solo: `/events`
- [ ] **TODO**: Limpiar contenido HTML antes de generar embeddings
  - Actualmente el summary tiene mucho ruido (navegaciÃ³n, scripts, etc)
  - Afecta la calidad de los embeddings y scores de bÃºsqueda
  - Mejorar extracciÃ³n en web_scraper.py antes de enrichment

### ðŸ€âš½ SPA Scraping (RECORDATORIO PERIÃ“DICO)
- [ ] **TODO**: Implementar soporte para sitios SPA (Single Page Application)
  - **Problema**: El scraper actual no ejecuta JavaScript, las noticias se cargan dinÃ¡micamente
  - **Soluciones posibles**:
    1. Activar `render_js: true` en ScraperTech para fuentes SPA
    2. Implementar Playwright/Puppeteer para headless browser scraping
    3. Detectar automÃ¡ticamente sitios SPA y usar mÃ©todo alternativo
  - **Fuentes afectadas conocidas** (misma plataforma Angular + Strapi):
    - Baskonia: https://www.baskonia.com/es/noticias (baloncesto)
    - Deportivo AlavÃ©s: https://deportivoalaves.com/es/noticias (fÃºtbol)
  - **Nota**: Ambos clubes usan la misma plataforma web (grupo Baskonia-AlavÃ©s)
  - **RECORDATORIO**: Preguntar al usuario si quiere priorizar esto cuando haya tiempo

### ðŸ“… Scheduled Publications (Frontend)
- [ ] **TODO**: Endpoint para revisar/confirmar publicaciones programadas
  - **Necesidad**: El frontend necesita un endpoint para ver las horas de publicaciÃ³n programadas
  - **Endpoint propuesto**: `GET /api/v1/articles/{article_id}/scheduled-publications`
  - **Respuesta**: Lista de publicaciones pendientes con target, platform_type, scheduled_for, status, social_hook
  - **Uso**: Confirmar visualmente las horas antes de que se ejecuten
  - **Tabla**: `scheduled_publications` (ya existe)

### Fase 1: Infraestructura
- [x] .env.example
- [x] .gitignore
- [x] requirements.md (actualizado con OpenRouter)
- [x] CLAUDE.md (este archivo)
- [ ] Dockerfile
- [ ] docker-compose.yml
- [ ] requirements.txt
- [ ] SQL schemas (Supabase)

### Fase 2: Core
- [ ] utils/logger.py (JSON logging)
- [ ] utils/config.py (Pydantic settings)
- [ ] core_ingest.py (guardrails, dedupe, carga)

### Fase 3: Conectores
- [ ] sources/web_scraper.py (BeautifulSoup + LLM)
- [ ] sources/twitter_scraper.py (scraper.tech API)
- [ ] sources/audio_transcriber.py (Whisper)
- [ ] sources/api_connectors.py (EFE, Reuters, WordPress)

### Fase 4: APIs
- [ ] server.py (FastAPI: /search, /aggregate, /ingest)
- [ ] scheduler.py (APScheduler: jobs + TTL cleanup)
- [ ] cli.py (add-client, add-task, list-*)

### Fase 5: CI/CD
- [ ] .github/workflows/deploy.yml (SSH â†’ VPS â†’ deploy)
- [ ] README.md (instrucciones de uso)
- [ ] Testing bÃ¡sico

## Testing

### Ejecutar Tests Unitarios

**IMPORTANTE**: Todos los cambios nuevos DEBEN incluir unit tests.

```bash
# Ejecutar todos los tests
./run_tests.sh

# O manualmente con pytest
python3 -m pytest tests/ -v

# Ejecutar un test especÃ­fico
python3 -m pytest tests/test_unified_content_enricher.py -v

# Ejecutar con coverage
python3 -m pytest tests/ --cov=utils --cov=sources --cov-report=html
```

### Estructura de Tests

```
/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_unified_content_enricher.py  # Content enrichment tests
â”œâ”€â”€ test_llm_client.py                 # (TODO)
â””â”€â”€ test_scraper_workflow.py           # (TODO)
```

### Escribir Tests

Usar `pytest` + `pytest-asyncio` para tests asÃ­ncronos:

```python
import pytest
from unittest.mock import Mock, AsyncMock, patch

@pytest.mark.asyncio
async def test_enrich_content():
    mock_llm = Mock()
    mock_llm.analyze_atomic = AsyncMock(return_value={...})
    
    with patch('utils.unified_content_enricher.get_llm_client', return_value=mock_llm):
        result = await enrich_content(...)
    
    assert result["category"] == "polÃ­tica"
```

## Deploy VPS (EasyPanel)

**IMPORTANTE**: EasyPanel usa `docker-compose.easypanel.yml`, NO `docker-compose.yml`

### VolÃºmenes persistentes configurados:
- `images_cache:/app/cache/images` - ImÃ¡genes generadas/cacheadas
- `fastembed_cache:/root/.cache/fastembed` - Modelos embeddings  
- `whisper_cache:/root/.cache/whisper` - Modelos audio

### SSH debug:
```bash
ssh semantika-vps
sudo docker logs -f ekimen_semantika-semantika-api-1
sudo docker exec ekimen_semantika-semantika-api-1 ls /app/cache/images/
```

**Deploy**: Auto desde GitHub push â†’ EasyPanel web restart

## Comandos Ãštiles

### Desarrollo Local (Mac M1)
```bash
# Primera vez
cp .env.example .env
# Editar .env con tus claves

# Levantar servicios
docker-compose up -d --build

# Ver logs
docker-compose logs -f semantika-api
docker-compose logs -f semantika-scheduler

# Acceder al API
curl http://localhost:8000/health

# Parar todo
docker-compose down
```

### ProducciÃ³n (VPS)

**Acceso SSH para Claude Code**:
```bash
# ConfiguraciÃ³n SSH (en ~/.ssh/config)
Host semantika-vps
    HostName api.ekimen.ai
    User ubuntu
    IdentityFile ~/.ssh/claude_vps_key

# Conectar (forma simple)
ssh semantika-vps

# O directamente
ssh -i ~/.ssh/claude_vps_key ubuntu@api.ekimen.ai

# Contenedores Docker (nombres completos)
# - ekimen_semantika-semantika-api-1 (FastAPI)
# - ekimen_semantika-semantika-scheduler-1 (APScheduler)
# - ekimen_semantika-qdrant-1 (Vector DB)
```

**Comandos Ãºtiles VPS**:
```bash
# Deploy automÃ¡tico
git push  # GitHub Actions hace el resto

# Deploy manual
ssh semantika-vps
cd ~/semantika  # o donde estÃ© el proyecto
git pull
sudo docker-compose up -d --build

# Ver logs en tiempo real
ssh semantika-vps "sudo docker logs -f ekimen_semantika-semantika-api-1"

# Reiniciar contenedor API
ssh semantika-vps "sudo docker restart ekimen_semantika-semantika-api-1"

# Ejecutar CLI en VPS
ssh semantika-vps "sudo docker exec ekimen_semantika-semantika-api-1 python cli.py list-clients"
```

### Operaciones de Base de Datos (Supabase)

**IMPORTANTE**: **SIEMPRE** usar los tools MCP de Supabase (`mcp__supabase__*`) para operaciones de BD en lugar de SSH+Python:

```python
# âœ… CORRECTO: Usar MCP tools
mcp__supabase__execute_sql(query="SELECT * FROM sources WHERE source_name = 'Medios Generalistas'")

# âŒ INCORRECTO: Usar SSH + Python inline
ssh semantika-vps "sudo docker exec ... python -c '...'"
```

**Razones**:
1. **MÃ¡s rÃ¡pido**: ConexiÃ³n directa a Supabase (no pasa por VPS)
2. **MÃ¡s seguro**: Usa credenciales Supabase nativas (no expone SSH)
3. **MÃ¡s simple**: No requiere escapar Python/JSON/SQL en bash
4. **Mejor logging**: Errores mÃ¡s claros y trazables

**Tools disponibles**:
- `mcp__supabase__execute_sql`: Ejecutar queries SQL (SELECT, UPDATE, DELETE)
- `mcp__supabase__apply_migration`: Aplicar migraciones DDL (CREATE, ALTER)
- `mcp__supabase__list_tables`: Listar tablas y esquemas
- `mcp__supabase__generate_typescript_types`: Generar tipos TypeScript
- `mcp__supabase__get_logs`: Ver logs de servicios (api, postgres, auth, etc.)
- `mcp__supabase__search_docs`: Buscar en documentaciÃ³n oficial de Supabase

**Ejemplos comunes**:

```python
# Actualizar schedule de una fuente
mcp__supabase__execute_sql(query="""
    UPDATE sources 
    SET schedule_config = jsonb_set(schedule_config, '{cron}', '"17:28"')
    WHERE source_name = 'Medios Generalistas'
    RETURNING source_name, schedule_config
""")

# Ver Ãºltimas ejecuciones
mcp__supabase__execute_sql(query="""
    SELECT execution_id, source_name, status, items_count, created_at
    FROM source_execution_log
    ORDER BY created_at DESC
    LIMIT 10
""")

# Contar context units por company
mcp__supabase__execute_sql(query="""
    SELECT company_id, COUNT(*) as total
    FROM press_context_units
    GROUP BY company_id
""")

# Ver fuentes descubiertas en pool
mcp__supabase__execute_sql(query="""
    SELECT source_name, url, status, relevance_score, discovered_at
    FROM discovered_sources
    WHERE company_id = '99999999-9999-9999-9999-999999999999'
    ORDER BY discovered_at DESC
    LIMIT 20
""")
```

### CLI Admin
```bash
# AÃ±adir cliente
docker exec -it semantika-api python cli.py add-client --name "Cliente X"

# AÃ±adir tarea
docker exec -it semantika-api python cli.py add-task \
  --client-id "uuid-X" \
  --type "web_llm" \
  --target "https://example.com" \
  --freq 60

# Listar clientes
docker exec -it semantika-api python cli.py list-clients
```

## Convenciones de CÃ³digo

### Imports
```python
# Standard library
import os
from typing import List, Dict, Any
from datetime import datetime

# Third party
from fastapi import FastAPI, HTTPException
from qdrant_client import QdrantClient
from supabase import create_client

# Local
from utils.logger import get_logger
from utils.config import settings
```

### Logging
```python
import json
from datetime import datetime

def log(level: str, service: str, action: str, **kwargs):
    print(json.dumps({
        "level": level,
        "timestamp": datetime.utcnow().isoformat(),
        "service": service,
        "action": action,
        **kwargs
    }))

# Uso
log("INFO", "core_ingest", "document_added",
    client_id="uuid-A", qdrant_id="uuid-1", source="web")
```

### Manejo de Errores
```python
try:
    result = await ingest_document(...)
    log("INFO", "scheduler", "ingest_success", task_id=task_id)
except Exception as e:
    log("ERROR", "scheduler", "ingest_failed",
        task_id=task_id, error=str(e))
    # NO re-raise en scheduler (continuar con otros jobs)
```

## Notas Importantes

1. **OpenRouter vs Ollama**: Usamos OpenRouter para reducir requisitos de hardware del VPS. No hay contenedor Ollama.

2. **Whisper**: Es pesado. Considerar hacerlo opcional o usar API externa en producciÃ³n.

3. **Qdrant Cloud**: Si el VPS es muy pequeÃ±o, considerar Qdrant Cloud en lugar de self-hosted.

4. **Supabase**: Es la Ãºnica fuente de verdad para configuraciÃ³n. No cachear en memoria sin TTL.

5. **GitHub Actions**: Requiere configurar SSH keys en GitHub Secrets antes del primer deploy.

## Referencias

- Arquitectura completa: `requirements.md`
- Qdrant Docs: https://qdrant.tech/documentation/
- OpenRouter Models: https://openrouter.ai/models
- Supabase Docs: https://supabase.com/docs
- ScraperTech API: https://scraper.tech/docs
