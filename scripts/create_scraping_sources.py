#!/usr/bin/env python3
"""Script to create two scraping sources for testing.

Creates:
1. Aiara Koudala - Multi-noticia (mÃºltiples noticias en una URL)
2. Prentsa Araba - Index (Ã¡rbol de noticias con enlaces)

Both run daily at 08:00
"""

import asyncio
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.supabase_client import get_supabase_client
from utils.scraper_helpers import create_scraping_source


async def main():
    """Create two scraping sources."""
    print("=" * 60)
    print("Creating Scraping Sources")
    print("=" * 60)
    
    supabase = get_supabase_client()
    
    # Get first company
    print("\nðŸ” Looking for company...")
    companies = supabase.client.table('companies').select('id, company_name').limit(1).execute()
    
    if not companies.data:
        print('âŒ No companies found. Please create a company first.')
        return
    
    company = companies.data[0]
    company_id = company['id']
    print(f'âœ… Using company: {company["company_name"]} ({company_id})')
    
    # Get first client for this company
    print("\nðŸ” Looking for client...")
    clients = supabase.client.table('clients').select('client_id').eq(
        'company_id', company_id
    ).limit(1).execute()
    
    if not clients.data:
        print('âŒ No clients found for this company. Please create a client first.')
        return
    
    client_id = clients.data[0]['client_id']
    print(f'âœ… Using client_id: {client_id}')
    
    # Source 1: Aiara Koudala (multi-noticia)
    print("\n" + "=" * 60)
    print("SOURCE 1: Aiara Koudala - Multi-noticia")
    print("=" * 60)
    print("URL: https://www.aiarakoudala.eus/es/noticias")
    print("Type: article (mÃºltiples noticias en la misma pÃ¡gina)")
    print("Schedule: Daily at 08:00")
    
    result1 = await create_scraping_source(
        company_id=company_id,
        client_id=client_id,
        url='https://www.aiarakoudala.eus/es/noticias',
        source_name='Aiara Koudala - Noticias',
        url_type='article',
        cron_schedule='08:00',
        is_active=True,
        description='Portal de noticias de Aiara Koudala - mÃºltiples noticias en la misma pÃ¡gina',
        tags=['scraping', 'aiara-koudala', 'euskadi', 'multi-noticia']
    )
    
    if result1['success']:
        print(f'\nâœ… Source created successfully!')
        print(f'   Source ID: {result1["source_id"]}')
        print(f'   Source Code: {result1["source"]["source_code"]}')
    else:
        print(f'\nâŒ Error creating source: {result1.get("error")}')
    
    # Source 2: Prentsa Araba (index)
    print("\n" + "=" * 60)
    print("SOURCE 2: Prentsa Araba - Ãndice de Noticias")
    print("=" * 60)
    print("URL: https://prentsa.araba.eus/es/noticias")
    print("Type: index (Ã¡rbol de noticias con enlaces a artÃ­culos)")
    print("Schedule: Daily at 08:00")
    
    result2 = await create_scraping_source(
        company_id=company_id,
        client_id=client_id,
        url='https://prentsa.araba.eus/es/noticias',
        source_name='Prentsa Araba - Ãndice',
        url_type='index',
        cron_schedule='08:00',
        is_active=True,
        description='Oficina de Prensa de Ãlava - Ã­ndice de noticias',
        tags=['scraping', 'prentsa-araba', 'alava', 'euskadi', 'index']
    )
    
    if result2['success']:
        print(f'\nâœ… Source created successfully!')
        print(f'   Source ID: {result2["source_id"]}')
        print(f'   Source Code: {result2["source"]["source_code"]}')
    else:
        print(f'\nâŒ Error creating source: {result2.get("error")}')
    
    # Summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    
    success_count = sum([1 for r in [result1, result2] if r['success']])
    print(f'Sources created: {success_count} / 2')
    
    if success_count > 0:
        print('\nðŸ“… Schedule:')
        print('   Both sources run daily at 08:00 UTC')
        print('\nðŸ”„ Scheduler:')
        print('   Sources will be picked up automatically')
        print('   (scheduler refreshes every 5 minutes)')
        print('\nðŸ“Š Monitoring:')
        print('   Check monitored_urls table for tracking')
        print('   Check url_change_log for change history')
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
