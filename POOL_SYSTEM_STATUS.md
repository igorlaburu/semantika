# Pool System - Estado de Implementaci√≥n

**Fecha actualizaci√≥n:** 8 Diciembre 2024  
**Status:** ‚úÖ Funcional (Discovery + Ingestion + Endpoints)

---

## üéØ Arquitectura Implementada

### Separaci√≥n Companies vs Pool

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMPANIES (Clientes periodistas - Privado)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ sources (tabla) ‚Üí scraper_workflow.py                      ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ monitored_urls (tracking URLs)                             ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ url_content_units (contenido scrapeado)                    ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ pgvector en Supabase (embeddings 768d)                     ‚îÇ
‚îÇ   - B√∫squedas privadas por company_id                      ‚îÇ
‚îÇ   - RLS habilitado                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ POOL (Sistema compartido - P√∫blico)                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ pool_discovery_config (tabla) ‚Üí Filtros geogr√°ficos        ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ workflows/discovery_flow.py (cada 3 d√≠as)                  ‚îÇ
‚îÇ   - GNews API ‚Üí Headlines geogr√°ficos                      ‚îÇ
‚îÇ   - Groq Compound ‚Üí B√∫squeda fuente original               ‚îÇ
‚îÇ   - extract_index_url() ‚Üí Encuentra p√°gina √≠ndice          ‚îÇ
‚îÇ   - analyze_press_room() ‚Üí Valida institutional source     ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ discovered_sources (tabla) ‚Üí Fuentes encontradas           ‚îÇ
‚îÇ   - Status: trial ‚Üí active ‚Üí inactive ‚Üí archived           ‚îÇ
‚îÇ   - M√©tricas: quality_score, content_count_7d              ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ workflows/ingestion_flow.py (cada hora)                    ‚îÇ
‚îÇ   - Scrape con WebScraper (sin tabla sources)              ‚îÇ
‚îÇ   - Enrich con LLM (category, atomic facts, quality)       ‚îÇ
‚îÇ   - Quality gate: >= 0.4                                   ‚îÇ
‚îÇ   ‚Üì                                                         ‚îÇ
‚îÇ Qdrant Pool collection (company_id="pool")                 ‚îÇ
‚îÇ   - Embeddings 768d (FastEmbed multilingual)               ‚îÇ
‚îÇ   - Deduplicaci√≥n autom√°tica (similarity > 0.98)           ‚îÇ
‚îÇ   - Todas las companies pueden consultar                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Componentes Implementados

### 1. Discovery System ‚úÖ

**Archivos:**
- `workflows/discovery_flow.py` - Orquestador principal
- `sources/discovery_connector.py` - LLM analysis + URL extraction
- `sources/gnews_client.py` - GNews API wrapper

**Funciones clave:**

#### `discovery_flow.py::execute_discovery_job()`
```python
# FLUJO:
# 1. Lee configs activas (pool_discovery_config)
# 2. Por cada config (√Ålava, Bizkaia...):
#    - Busca noticias en GNews (query geogr√°fico)
#    - Sample 5% de art√≠culos
#    - Por cada headline:
#      a. Groq Compound ‚Üí Busca fuente original
#      b. extract_index_url() ‚Üí Encuentra √≠ndice (/news, /sala-prensa)
#      c. analyze_press_room() ‚Üí Valida + metadata
#      d. Guarda en discovered_sources

# Scheduling: Cada 3 d√≠as a las 8:00 UTC
# Job: pool_discovery_job() en scheduler.py
```

#### `discovery_connector.py::extract_index_url()` ‚úÖ NUEVO
```python
# PROP√ìSITO:
# Convertir URL de art√≠culo espec√≠fico ‚Üí URL del √≠ndice de noticias
# 
# INPUT: https://irekia.eus/es/events/106714-titulo-largo
# OUTPUT: https://irekia.eus/es/events
#
# M√âTODO:
# 1. Fetch HTML completo (sin filtros)
# 2. Env√≠a HTML al LLM (groq_fast)
# 3. LLM analiza breadcrumbs, navigation, URL structure
# 4. LLM extrae href del √≠ndice o infiere quitando slug
# 
# RETURN: {
#   "index_url": "https://...",
#   "confidence": 0.9,
#   "method": "breadcrumb_link" | "navigation_link" | "url_inference"
# }
```

#### `discovery_connector.py::analyze_press_room()` ‚úÖ
```python
# PROP√ìSITO:
# Validar que una URL es sala de prensa institucional
#
# INPUT: URL del √≠ndice (NO art√≠culo espec√≠fico)
# OUTPUT: {
#   "is_press_room": true,
#   "confidence": 0.8,
#   "org_name": "Gobierno Vasco",
#   "contact_email": "prensa@euskadi.eus",
#   "estimated_quality": 0.7,
#   "notes": "Sala de prensa activa con comunicados regulares"
# }
#
# TRACKING: SYSTEM organization (88044361-8529-46c8-8196-d1345ca7bbe8)
```

**Tablas DB:**

#### `pool_discovery_config`
```sql
CREATE TABLE pool_discovery_config (
    config_id UUID PRIMARY KEY,
    geographic_area TEXT NOT NULL,           -- "√Ålava", "Bizkaia"
    search_query TEXT NOT NULL,              -- "Vitoria"
    gnews_lang TEXT DEFAULT 'es',
    gnews_country TEXT DEFAULT 'es',
    max_articles INT DEFAULT 100,
    sample_rate FLOAT DEFAULT 0.05,          -- 5%
    excluded_domains TEXT[] DEFAULT '{}',
    target_source_types TEXT[] DEFAULT ARRAY['press_room', 'institutional'],
    is_active BOOLEAN DEFAULT true,
    priority INT DEFAULT 1,
    created_by UUID REFERENCES organizations(id)  -- SYSTEM org
);

-- Estado actual: 1 config activo (√Ålava)
```

#### `discovered_sources`
```sql
-- Fuentes encontradas autom√°ticamente
{
  "source_id": "uuid",
  "source_name": "Gobierno Vasco",
  "url": "https://irekia.euskadi.eus/es/events",  -- URL √çNDICE (no art√≠culo)
  "status": "trial",  -- trial ‚Üí active ‚Üí inactive ‚Üí archived
  "relevance_score": 0.8,
  "avg_quality_score": 0.7,
  "content_count_7d": 0,
  "company_id": "00000000-0000-0000-0000-000000000999",  -- Pool UUID
  "config": {
    "original_source_url": "https://.../106714-...",  -- Art√≠culo original
    "index_url": "https://.../events",                 -- √çndice extra√≠do
    "index_extraction_method": "breadcrumb_link",
    "index_extraction_confidence": 0.9,
    "discovery_config_id": "uuid",
    "geographic_area": "√Ålava"
  }
}

-- Estado actual: 1 source descubierta (Irekia Gobierno Vasco)
```

---

### 2. Ingestion System ‚úÖ

**Archivos:**
- `workflows/ingestion_flow.py` - Scraping + enrichment + Qdrant
- `sources/web_scraper.py` - HTML scraping (usado por Pool)
- `utils/pool_client.py` - Qdrant Pool operations

**Funciones clave:**

#### `ingestion_flow.py::execute_ingestion_job()`
```python
# FLUJO:
# 1. Get active sources (discovered_sources WHERE status IN ('trial', 'active'))
# 2. Por cada source:
#    a. Scrape con WebScraper (NO usa tabla sources)
#    b. Enrich con LLM (title, summary, category, atomic_facts, quality_score)
#    c. Quality gate: quality_score >= 0.4
#    d. Ingest a Qdrant via pool_client.ingest_to_pool()
#    e. Update stats en discovered_sources

# Scheduling: Cada hora
# Job: pool_ingestion_job() en scheduler.py
```

#### `pool_client.py::ingest_to_pool()` ‚úÖ
```python
# PROP√ìSITO:
# Ingerir contenido enriquecido a Qdrant Pool collection
#
# FEATURES:
# - Genera embedding 768d (FastEmbed multilingual)
# - Deduplicaci√≥n autom√°tica (similarity > 0.98)
# - Quality threshold: >= 0.4
# - Collection: 'pool' (company_id="pool")
#
# PAYLOAD Qdrant:
# {
#   "company_id": "pool",
#   "source_id": "uuid",
#   "title": "...",
#   "content": "...",  # Truncado 5000 chars
#   "category": "econom√≠a",
#   "tags": [...],
#   "quality_score": 0.75,
#   "atomic_statements": [...],  # M√°x 20
#   "published_at": "2024-12-08T...",
#   "ingested_at": "2024-12-08T...",
#   "source_name": "Gobierno Vasco",
#   "source_code": "www_irekia_euskadi_eus"
# }
```

#### `pool_client.py::search()` ‚úÖ
```python
# B√∫squeda sem√°ntica en Pool con filtros:
# - categories: ["econom√≠a", "pol√≠tica"]
# - date_from / date_to
# - min_quality: 0.6
# - tags: ["subvenciones"]
# - score_threshold: 0.7
```

**Tablas DB:**

#### `companies` (Pool company)
```sql
-- UUID especial para Pool
{
  "id": "00000000-0000-0000-0000-000000000999",
  "company_code": "pool",
  "company_name": "Pool (Sistema compartido)",
  "tier": "unlimited",
  "settings": {
    "unlimited_usage": true,
    "store_in_qdrant": true
  }
}
```

#### `organizations` (SYSTEM org)
```sql
-- Para tracking LLM del sistema Pool
{
  "id": "88044361-8529-46c8-8196-d1345ca7bbe8",
  "slug": "system",
  "name": "System Pool Operations",
  "company_id": null,
  "is_active": true
}
```

---

### 3. API Endpoints ‚úÖ

#### Discovery & Management

**`GET /pool/system/health`** ‚úÖ
```
Auth: X-System-Key
Returns: {
  "status": "healthy",
  "pool_stats": {
    "total_context_units": 0,
    "collection_name": "pool",
    "total_sources": 1,
    "sources_by_status": {"trial": 1}
  }
}
```

**`GET /pool/system/stats`** ‚úÖ
```
Auth: X-System-Key
Returns: {
  "total_context_units": 0,
  "collection_name": "pool",
  "avg_source_relevance": 0.8,
  "avg_source_quality": 0.7
}
```

**`GET /pool/sources`** ‚úÖ
```
Auth: X-API-Key
Params: status, limit
Returns: {
  "sources": [{
    "source_id": "uuid",
    "source_name": "Gobierno Vasco",
    "url": "https://irekia.euskadi.eus/es/events",
    "status": "trial",
    "relevance_score": 0.8,
    "avg_quality_score": 0.7,
    "config": {...}
  }]
}
```

#### Search & Context

**`POST /pool/search`** ‚úÖ
```
Auth: X-API-Key
Body: {
  "query": "Vitoria inversi√≥n industrial",
  "limit": 10,
  "filters": {
    "category": "econom√≠a",
    "date_from": "2024-01-01"
  },
  "score_threshold": 0.7
}
Returns: {
  "results": [...],
  "total": 0,
  "query_time_ms": 89.9
}
```

**`GET /pool/context/{context_id}`** ‚úÖ
```
Auth: X-API-Key
Returns: {
  "id": "uuid",
  "title": "...",
  "content": "...",
  "category": "econom√≠a",
  "tags": [...],
  "quality_score": 0.75,
  "atomic_statements": [...],
  "source_name": "Gobierno Vasco"
}
```

**`POST /pool/adopt`** ‚úÖ
```
Auth: JWT (user token)
Body: {
  "context_id": "uuid",
  "target_organization_id": "user-org-uuid"
}
Purpose: Copiar context unit del Pool a espacio privado del usuario
```

#### ‚ùå Endpoint Faltante

**`GET /pool/context-units`** (listar con filtros)
```
# TODO: Implementar endpoint de listado
# Similar a /pool/search pero sin query text
# Filtros: category, date_from, date_to, min_quality, limit, offset
```

---

## üîÑ Scheduling (scheduler.py)

```python
# Pool discovery job - Cada 3 d√≠as a las 8:00 UTC
scheduler.add_job(
    pool_discovery_job,
    trigger=CronTrigger(hour=8, minute=0, day='*/3'),
    id="pool_discovery"
)

# Pool ingestion job - Cada hora
scheduler.add_job(
    pool_ingestion_job,
    trigger=IntervalTrigger(hours=1),
    id="pool_ingestion"
)
```

---

## üìà Estado Actual (8 Dic 2024)

### Discovered Sources
| Source | URL | Status | Quality | Last Scraped |
|--------|-----|--------|---------|-------------|
| Gobierno Vasco (Irekia) | `irekia.euskadi.eus/es/events` | trial | 0.7 | Nunca |

### Qdrant Pool Collection
- **Total points:** 0 (vac√≠a)
- **Vector size:** 768d
- **Collection name:** `pool`

### Next Steps
1. ‚úÖ Discovery encontr√≥ 1 fuente (Irekia)
2. ‚è≥ Ingestion debe scrapear e ingestar (pr√≥xima hora)
3. ‚è≥ Validar que aparece contenido en `/pool/search`

---

## üêõ Problemas Conocidos

### 1. extract_index_url() - Sin probar a√∫n
- ‚úÖ Implementado
- ‚è≥ Pendiente: Probar con m√°s URLs reales
- **Pr√≥ximo test:** Pr√≥xima ejecuci√≥n discovery (cada 3 d√≠as)

### 2. Ingestion flow - Error en enrich_content
- ‚úÖ Fixed: Removido `organization_id` parameter
- ‚è≥ Pendiente: Validar ingestion completa (pr√≥xima hora)

### 3. WebScraper - Puede no extraer contenido
- **S√≠ntoma:** `scrape_url()` devuelve lista vac√≠a
- **Causa:** P√°gina compleja (mucho JS, anti-scraping)
- **Soluci√≥n futura:** Usar scraping service (ScraperAPI, etc.)

---

## üìù Mejoras Futuras

### Corto Plazo (1-2 semanas)
1. [ ] Implementar `GET /pool/context-units` (listing endpoint)
2. [ ] A√±adir m√°s configs geogr√°ficos (Bizkaia, Gipuzkoa)
3. [ ] Lifecycle management (trial ‚Üí active ‚Üí archived)
4. [ ] Metrics dashboard (/pool/system/metrics)

### Medio Plazo (1 mes)
1. [ ] Relevance scoring autom√°tico
2. [ ] Source quality evaluation (hist√≥rico)
3. [ ] Deduplicaci√≥n cross-source
4. [ ] Notification system (nuevas fuentes high-quality)

### Largo Plazo (3 meses)
1. [ ] Frontend UI para discovered sources
2. [ ] Manual approval workflow (humano valida sources)
3. [ ] A/B testing discovery strategies
4. [ ] Export discovered sources to JSON/CSV

---

## üìö Documentaci√≥n Relacionada

- `plan_desarrollo_discovery.md` - Plan original (obsoleto parcialmente)
- `reflexiones_sobre_pgvector_y_qdrant_fuentes-propias-y-pool.md` - Arquitectura Pool
- `CLAUDE.md` - Gu√≠a desarrollo general

---

**√öltima actualizaci√≥n:** 8 Diciembre 2024  
**Autor:** Claude Code + Igor  
**Status:** ‚úÖ Sistema funcional, esperando primera ingestion
