# Sistema de Monitoreo de Subvenciones DFA - ImplementaciÃ³n Completa

**Fecha**: 2025-11-26  
**Usuario**: igor@gako.ai  
**URL Objetivo**: https://egoitza.araba.eus/es/-/tr-solicitar-ayudas-forestales  
**Schedule**: Diario a las 8:00 AM UTC

---

## âœ… ImplementaciÃ³n Completada

**NOTA IMPORTANTE**: El source DFA usa `source_type='api'` con `config.connector_type='dfa_subsidies'` porque la tabla `sources` tiene un constraint que solo permite ciertos source_types predefinidos. El scheduler detecta el connector_type y enruta al monitor DFA correspondiente.

### 1. Base de Datos (SQL Migrations)

#### `sql/migrations/003_create_web_context_units.sql`
- **Tabla**: `web_context_units`
- **Estructura**: Similar a `press_context_units` pero para contenido web
- **CaracterÃ­sticas**:
  - Embeddings 768d (FastEmbed multilingual)
  - Versioning (campo `version`, `replaced_by_id`, `is_latest`)
  - Change tracking (content_hash, simhash)
  - RLS multi-tenant
  - FunciÃ³n `match_web_context_units()` para bÃºsqueda semÃ¡ntica

#### `sql/migrations/004_create_dfa_subsidies_source.sql`
- **Source**: Configurado para company `gako` (igor@gako.ai)
- **Config**:
  ```json
  {
    "target_url": "https://egoitza.araba.eus/es/-/tr-solicitar-ayudas-forestales",
    "change_detection": {
      "method": "simhash",
      "simhash_threshold": 0.90
    },
    "pdf_extraction": {
      "enabled": true,
      "max_file_size_mb": 10,
      "summarize_with_llm": true
    }
  }
  ```
- **Schedule**: Cron `0 8 * * *` (8:00 AM UTC)

---

### 2. Dependencias AÃ±adidas

**requirements.txt**:
```
PyPDF2==3.0.1        # ExtracciÃ³n de texto de PDFs
pdfplumber==0.10.4   # Fallback para PDFs complejos
jinja2==3.1.3        # Templates de informes MD
simhash==2.1.2       # Ya estaba (detecciÃ³n de cambios)
```

---

### 3. Componentes Implementados

#### `utils/pdf_extractor.py`
**Funcionalidad**:
- Descarga PDFs con lÃ­mite de tamaÃ±o (10MB) y timeout (30s)
- ExtracciÃ³n de texto multi-mÃ©todo:
  1. PyPDF2 (rÃ¡pido, PDFs text-based)
  2. pdfplumber (fallback, layouts complejos)
- Resumen LLM (Llama 3.3 70B) en bullet points
- Procesamiento paralelo de mÃºltiples PDFs

**MÃ©todos clave**:
```python
async def download_pdf(url) -> (bytes, error)
def extract_text(pdf_bytes) -> (text, errors)
async def summarize_with_llm(text) -> List[str]  # Bullet points
async def process_pdf(url) -> Dict  # Pipeline completo
```

#### `utils/md_report_generator.py`
**Funcionalidad**:
- Templates Jinja2 para informes estructurados
- Formato consistente para subvenciones

**Template sections**:
```markdown
# TÃ­tulo
## ðŸ“… Plazos de PresentaciÃ³n
## ðŸ“‹ MetodologÃ­a de PresentaciÃ³n
## ðŸ“„ DocumentaciÃ³n a Presentar
  ### 1. Documento
  - Enlace
  - Resumen (bullets del PDF)
## ðŸ’° Solicitudes de Pago
## ðŸ“Œ InformaciÃ³n Adicional
```

**MÃ©todo clave**:
```python
def generate_subsidy_report(
    titulo, url, plazos, metodologia, 
    documentacion, solicitudes_pago
) -> str  # Markdown
```

#### `workflows/subsidy_extraction_workflow.py`
**Funcionalidad**:
- Workflow especializado para extracciÃ³n de subvenciones
- Hereda de `BaseWorkflow`

**Pipeline**:
1. **LLM Extraction** â†’ JSON estructurado con:
   - TÃ­tulo
   - Plazos (estado, fecha_inicio, fecha_fin)
   - MetodologÃ­a (descripciÃ³n)
   - DocumentaciÃ³n (lista de {titulo, url, descripcion})
   - Solicitudes de pago
2. **PDF Processing** â†’ Descarga y resume todos los PDFs en paralelo
3. **MD Report** â†’ Genera informe Markdown con template
4. **Context Unit** â†’ Prepara datos para `ingest_web_context_unit()`

**MÃ©todo clave**:
```python
async def generate_context_unit(source_content) -> Dict
```

#### `sources/dfa_subsidies_monitor.py`
**Funcionalidad**:
- Monitor especializado para pÃ¡gina DFA
- DetecciÃ³n de cambios con SimHash (threshold 0.90)

**Pipeline**:
1. **Fetch HTML** â†’ Descarga pÃ¡gina actual
2. **Compare SimHash** â†’ Compara con snapshot anterior
   - `identical` / `trivial` â†’ Skip
   - `minor_update` / `major_update` â†’ Process
3. **Process Updates** â†’ Ejecuta `SubsidyExtractionWorkflow`
4. **Save to DB** â†’ Llama `ingest_web_context_unit()`
5. **Save Snapshot** â†’ Guarda hashes para prÃ³xima comparaciÃ³n

**MÃ©todos clave**:
```python
async def fetch_page(url) -> str
async def check_for_updates(source, company) -> bool
```

#### `utils/unified_context_ingester.py` (Actualizado)
**Funcionalidad aÃ±adida**:
- Nueva funciÃ³n `ingest_web_context_unit()` para `web_context_units`
- LÃ³gica de versioning y reemplazo
- Genera content_hash y simhash automÃ¡ticamente

**MÃ©todo nuevo**:
```python
async def ingest_web_context_unit(
    raw_text: str,
    title=None, summary=None, tags=None,
    company_id, source_type, source_id,
    replace_previous=True  # Reemplaza versiÃ³n anterior
) -> Dict
```

**LÃ³gica de versioning**:
- Si `replace_previous=True`:
  - Busca registro existente con `is_latest=True`
  - Si existe â†’ UPDATE (incrementa version)
  - Si no existe â†’ INSERT
- Si `replace_previous=False`:
  - Siempre INSERT (nueva versiÃ³n)

#### `scheduler.py` (Actualizado)
**Funcionalidad aÃ±adida**:
- Nuevo caso en `execute_source_task()` para `source_type="dfa_subsidies"`

**LÃ³gica**:
```python
if source_type == "dfa_subsidies":
    monitor = get_dfa_subsidies_monitor()
    changes_detected = await monitor.check_for_updates(source, company)
    # Log execution
    # Update stats
```

**IntegraciÃ³n**:
- El scheduler **ya lee dinÃ¡micamente** todas las sources activas
- Ejecuta cada source segÃºn su `schedule_config`
- No requiere hardcodear el job DFA

---

## ðŸ”§ ConfiguraciÃ³n y Deployment

### Paso 1: Ejecutar Migraciones SQL âœ… COMPLETADO

```bash
# Migraciones ejecutadas vÃ­a MCP Supabase:
# âœ… Migration 003: create_web_context_units - Tabla creada
# âœ… Migration 004: create_dfa_subsidies_source - Source creado

# Source ID: 58b0f22e-ad7f-4dbe-9086-027307970070
# Source Type: api (connector_type: dfa_subsidies)
# Company: GAKO AI PRUEBAS (2cfa7d05-d754-4b78-a426-a117af1616d8)
```

### Paso 2: Instalar Dependencias

```bash
cd /Users/igor/Documents/semantika
pip install -r requirements.txt
```

### Paso 3: Verificar Source Configurado âœ… COMPLETADO

```sql
-- Verificar source DFA
SELECT 
    source_id, source_name, source_type, is_active,
    config->>'connector_type' as connector_type,
    config->>'target_url' as url,
    schedule_config->>'cron_expression' as cron
FROM sources
WHERE source_code = 'dfa_subsidies_monitor';

-- Resultado:
-- source_id: 58b0f22e-ad7f-4dbe-9086-027307970070
-- source_type: api
-- connector_type: dfa_subsidies
-- is_active: true
-- cron: 0 8 * * *
```

### Paso 4: Deploy a ProducciÃ³n

```bash
# Commit cambios
git add .
git commit -m "Add DFA subsidies monitoring system"
git push

# GitHub Actions desplegarÃ¡ automÃ¡ticamente
```

---

## ðŸ“Š Flujo de EjecuciÃ³n

### EjecuciÃ³n Diaria (8:00 AM UTC)

```
1. Scheduler lee source con source_type='dfa_subsidies'
2. Ejecuta execute_source_task(source)
   â†“
3. DFASubsidiesMonitor.check_for_updates()
   â†“
4. Fetch HTML de https://egoitza.araba.eus/...
   â†“
5. Calcular SimHash y comparar con snapshot anterior
   â†“
6. SI cambios significativos (similarity < 0.90):
   â†“
   7. SubsidyExtractionWorkflow.process_content()
      â†“
      8. LLM extrae JSON estructurado
      9. Descarga y resume PDFs (paralelo)
      10. Genera informe Markdown
      â†“
   11. ingest_web_context_unit()
      - Genera embedding (768d)
      - Calcula content_hash y simhash
      - UPDATE registro existente (versioning)
      â†“
   12. Save snapshot (hashes para prÃ³xima comparaciÃ³n)
   
   RESULTADO: web_context_units actualizado
   
7. ELSE (sin cambios):
   - Log "Sin cambios significativos"
   - No procesa
```

---

## ðŸŽ¯ CaracterÃ­sticas Clave

### SimHash Change Detection
- **Inmune a cambios triviales**:
  - Timestamps actualizados
  - Banners/ads rotados
  - Cambios CSS/layout menores
- **Detecta cambios relevantes**:
  - Plazos modificados
  - Nuevos documentos aÃ±adidos
  - Estado cambiado (abierto/cerrado)

### PDF Processing Inteligente
- **Multi-mÃ©todo**: PyPDF2 â†’ pdfplumber (fallback)
- **Parallel downloads**: Hasta 3 PDFs simultÃ¡neos
- **Size limit**: 10MB mÃ¡ximo
- **LLM summaries**: Bullet points concisos
- **Error handling**: ContinÃºa si algÃºn PDF falla

### Versioning System
- **is_latest flag**: Solo una versiÃ³n activa por source
- **version number**: Incrementa en cada update
- **replaced_by_id**: Chain de versiones histÃ³ricas
- **Queries**: Filtrar por `WHERE is_latest = TRUE`

### Multi-tenant Isolation
- **RLS policies**: AutomÃ¡tico por company_id
- **Embeddings por company**: DeduplicaciÃ³n aislada
- **Logs separados**: Por client_id y company_id

---

## ðŸ§ª Testing Manual

### Test 1: Verificar Source Configurado

```bash
# SSH al servidor
ssh usuario@api.ekimen.ai

# Ver source DFA
docker exec -it semantika-api python -c "
from utils.supabase_client import get_supabase_client
import asyncio

async def test():
    supabase = get_supabase_client()
    result = supabase.client.table('sources')\
        .select('*')\
        .eq('source_type', 'dfa_subsidies')\
        .execute()
    print(result.data)

asyncio.run(test())
"
```

### Test 2: Ejecutar Manualmente

```bash
# Ejecutar monitor una vez
docker exec -it semantika-api python -c "
from sources.dfa_subsidies_monitor import get_dfa_subsidies_monitor
from utils.supabase_client import get_supabase_client
import asyncio

async def test():
    supabase = get_supabase_client()
    
    # Get source
    source = supabase.client.table('sources')\
        .select('*')\
        .eq('source_type', 'dfa_subsidies')\
        .single()\
        .execute().data
    
    # Get company
    company = supabase.client.table('companies')\
        .select('*')\
        .eq('id', source['company_id'])\
        .single()\
        .execute().data
    
    # Run monitor
    monitor = get_dfa_subsidies_monitor()
    result = await monitor.check_for_updates(source, company)
    
    print(f'Changes detected: {result}')

asyncio.run(test())
"
```

### Test 3: Verificar web_context_units

```sql
-- Ver Ãºltima versiÃ³n guardada
SELECT 
    id, title, category, version, is_latest,
    created_at, updated_at,
    LENGTH(raw_text) as report_length,
    tags
FROM web_context_units
WHERE source_type = 'dfa_subsidies'
AND is_latest = TRUE
ORDER BY updated_at DESC
LIMIT 1;
```

---

## ðŸ“ PrÃ³ximos Pasos

### Pendiente

1. **Unit Tests** (`tests/test_dfa_subsidies_monitor.py`):
   - Test SimHash detection
   - Test PDF extraction
   - Test MD report generation
   - Mock LLM responses

2. **Integration Test** (`tests/integration/test_dfa_end_to_end.py`):
   - Test completo con HTML de ejemplo
   - Verificar base de datos
   - Verificar versioning

3. **Monitoring**:
   - Alertas si falla extracciÃ³n
   - Dashboard con histÃ³rico de cambios
   - Notificaciones a igor@gako.ai cuando hay updates

### Mejoras Futuras

- **OCR para PDFs escaneados**: Tesseract si los PDFs son imÃ¡genes
- **Diff visualization**: Mostrar quÃ© cambiÃ³ exactamente
- **Email notifications**: Enviar informe cuando hay cambios
- **API endpoint**: GET /api/v1/subsidies/dfa/latest

---

## ðŸ› Troubleshooting

### Error: "Company 'gako' not found"
```sql
-- Verificar si existe
SELECT * FROM companies WHERE company_code = 'gako';

-- Si no existe, crear primero:
INSERT INTO companies (company_name, company_code, is_active)
VALUES ('Gako', 'gako', TRUE);
```

### Error: "Table web_context_units does not exist"
```bash
# Ejecutar migraciÃ³n 003
# En Supabase SQL Editor, copiar contenido de:
# sql/migrations/003_create_web_context_units.sql
```

### Error: "PDF download timeout"
```python
# Ajustar timeout en source config:
UPDATE sources
SET config = jsonb_set(
    config,
    '{pdf_extraction,timeout_seconds}',
    '60'
)
WHERE source_type = 'dfa_subsidies';
```

### Error: "SimHash library not installed"
```bash
pip install simhash==2.1.2
```

---

## ðŸ“š Referencias

- **SimHash**: `utils/content_hasher.py` - Multi-tier change detection
- **Workflow Factory**: `workflows/workflow_factory.py` - Registro de workflows
- **LLM Client**: `utils/llm_client.py` - OpenRouter integration
- **Embedding Generator**: `utils/embedding_generator.py` - 768d FastEmbed

---

**Estado**: âœ… ImplementaciÃ³n completa  
**PrÃ³ximo deploy**: Ejecutar migraciones SQL y push a producciÃ³n
